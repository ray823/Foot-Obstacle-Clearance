{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0feb1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import scipy.io\n",
    "from mat4py import loadmat\n",
    "import keras\n",
    "from keras.callbacks import History,EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers import Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.layers import Reshape\n",
    "import time\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45844247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC10\\Trial40\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC10\\Trial41\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC10\\Trial49\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC10\\Trial50\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC20\\Trial100\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC20\\Trial109\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC20\\Trial110\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC20\\Trial99\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC30\\Trial113\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC30\\Trial114\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC30\\Trial116\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E01\\OC30\\Trial121\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC10\\Trial110\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC10\\Trial111\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC10\\Trial115\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC10\\Trial116\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC20\\Trial152\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC20\\Trial153\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC20\\Trial157\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC20\\Trial158\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC30\\Trial173\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC30\\Trial174\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC30\\Trial177\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E02\\OC30\\Trial178\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC10\\Trial20\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC10\\Trial21\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC10\\Trial25\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC10\\Trial26\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC20\\Trial71\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC20\\Trial72\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC20\\Trial77\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC20\\Trial78\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC30\\Trial81\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC30\\Trial82\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC30\\Trial87\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E03\\OC30\\Trial88\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC10\\Trial45\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC10\\Trial46\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC10\\Trial50\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC10\\Trial52\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC20\\Trial70\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC20\\Trial71\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC20\\Trial74\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC20\\Trial75\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC30\\Trial90\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC30\\Trial91\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC30\\Trial98\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E04\\OC30\\Trial99\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC10\\Trial41\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC10\\Trial42\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC10\\Trial45\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC10\\Trial46\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC20\\Trial81\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC20\\Trial82\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC20\\Trial86\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC20\\Trial87\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC30\\Trial110\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC30\\Trial111\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC30\\Trial114\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E05\\OC30\\Trial115\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC10\\Trial24\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC10\\Trial25\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC10\\Trial30\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC10\\Trial32\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC20\\Trial72\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC20\\Trial74\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC20\\Trial76\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC20\\Trial77\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC30\\Trial130\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC30\\Trial132\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC30\\Trial138\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E06\\OC30\\Trial142\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC10\\Trial46\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC10\\Trial47\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC10\\Trial48\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC10\\Trial50\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC20\\Trial67\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC20\\Trial69\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC20\\Trial73\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC20\\Trial75\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC30\\Trial100\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC30\\Trial104\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC30\\Trial105\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E07\\OC30\\Trial97\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC10\\Trial54\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC10\\Trial55\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC10\\Trial56\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC10\\Trial60\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC20\\Trial78\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC20\\Trial81\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC20\\Trial83\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC20\\Trial88\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC30\\Trial102\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC30\\Trial105\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC30\\Trial111\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E09\\OC30\\Trial112\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC10\\Trial55\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC10\\Trial56\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC10\\Trial58\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC10\\Trial60\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC20\\Trial85\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC20\\Trial87\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC20\\Trial92\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC20\\Trial93\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC30\\Trial110\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC30\\Trial112\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC30\\Trial120\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E10\\OC30\\Trial122\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC10\\Trial47\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC10\\Trial48\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC10\\Trial55\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC10\\Trial56\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC20\\Trial70\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC20\\Trial71\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC20\\Trial74\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC20\\Trial75\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC30\\Trial127\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC30\\Trial128\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC30\\Trial134\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E11\\OC30\\Trial135\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC10\\Trial27\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC10\\Trial30\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC10\\Trial34\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC10\\Trial35\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC20\\Trial101\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC20\\Trial82\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC20\\Trial94\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC20\\Trial96\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC30\\Trial121\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC30\\Trial122\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC30\\Trial123\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E12\\OC30\\Trial126\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC10\\Trial49\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC10\\Trial50\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC10\\Trial52\\Results.mat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC10\\Trial54\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC20\\Trial87\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC20\\Trial88\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC20\\Trial90\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC20\\Trial91\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC30\\Trial107\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC30\\Trial108\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC30\\Trial113\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E13\\OC30\\Trial116\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC10\\Trial48\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC10\\Trial49\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC10\\Trial50\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC10\\Trial52\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC20\\Trial58\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC20\\Trial59\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC20\\Trial60\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC20\\Trial62\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC30\\Trial92\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC30\\Trial93\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC30\\Trial95\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E14\\OC30\\Trial96\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC10\\Trial36\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC10\\Trial38\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC10\\Trial39\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC10\\Trial45\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC20\\Trial59\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC20\\Trial60\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC20\\Trial65\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC20\\Trial66\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC30\\Trial111\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC30\\Trial112\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC30\\Trial115\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E15\\OC30\\Trial117\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC10\\Trial36\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC10\\Trial37\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC10\\Trial39\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC10\\Trial40\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC20\\Trial72\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC20\\Trial73\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC20\\Trial79\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC20\\Trial80\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC30\\Trial122\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC30\\Trial123\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC30\\Trial126\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E16\\OC30\\Trial128\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC10\\Trial50\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC10\\Trial52\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC10\\Trial54\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC10\\Trial57\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC20\\Trial74\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC20\\Trial75\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC20\\Trial80\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC20\\Trial81\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC30\\Trial127\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC30\\Trial129\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC30\\Trial138\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E17\\OC30\\Trial139\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC10\\Trial73\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC10\\Trial74\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC10\\Trial77\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC10\\Trial87\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC20\\Trial100\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC20\\Trial97\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC20\\Trial98\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC20\\Trial99\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC30\\Trial162\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC30\\Trial163\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC30\\Trial164\\Results.mat\n",
      "C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/E18\\OC30\\Trial165\\Results.mat\n",
      "[[[-1.85592973e+01 -1.90803045e+01 -1.87862629e+01 ... -3.76789879e+00\n",
      "   -6.07677844e+00 -7.29325804e+00]\n",
      "  [-3.84781590e+00 -3.20896126e+00 -2.57869571e+00 ... -5.51788470e+00\n",
      "   -4.58697359e+00 -3.49665725e+00]\n",
      "  [-2.93677355e+00 -2.57873037e+00 -2.59398353e+00 ... -1.42303724e+00\n",
      "   -1.92032046e+00 -2.18092033e+00]\n",
      "  ...\n",
      "  [ 2.22474176e+02  2.22474176e+02  2.22474176e+02 ...  2.22474176e+02\n",
      "    2.22474176e+02  2.22474176e+02]\n",
      "  [ 3.32924076e+02  3.32924076e+02  3.32924076e+02 ...  3.32924076e+02\n",
      "    3.32924076e+02  3.32924076e+02]\n",
      "  [ 3.25739195e+02  3.25739195e+02  3.25739195e+02 ...  3.25739195e+02\n",
      "    3.25739195e+02  3.25739195e+02]]\n",
      "\n",
      " [[-9.70035625e+00 -1.00515583e+01 -9.34058876e+00 ... -3.93716714e+00\n",
      "   -6.02556767e+00 -7.27759620e+00]\n",
      "  [-5.75983350e+00 -4.63195053e+00 -4.14533811e+00 ... -6.21241577e+00\n",
      "   -5.67477095e+00 -4.99240690e+00]\n",
      "  [-2.12273165e+00 -2.18466607e+00 -2.01615504e+00 ... -7.76796907e-01\n",
      "   -1.63168535e+00 -2.36125927e+00]\n",
      "  ...\n",
      "  [ 2.22474176e+02  2.22474176e+02  2.22474176e+02 ...  2.22474176e+02\n",
      "    2.22474176e+02  2.22474176e+02]\n",
      "  [ 3.32924076e+02  3.32924076e+02  3.32924076e+02 ...  3.32924076e+02\n",
      "    3.32924076e+02  3.32924076e+02]\n",
      "  [ 3.25739195e+02  3.25739195e+02  3.25739195e+02 ...  3.25739195e+02\n",
      "    3.25739195e+02  3.25739195e+02]]\n",
      "\n",
      " [[-1.15720778e+01 -1.18184826e+01 -1.10173601e+01 ... -3.21166943e+00\n",
      "   -5.00608685e+00 -5.84269981e+00]\n",
      "  [-5.24339811e+00 -4.27596528e+00 -3.80558949e+00 ... -5.52233219e+00\n",
      "   -4.49130297e+00 -3.24554597e+00]\n",
      "  [-9.02272083e-01 -1.40903206e+00 -1.35969266e+00 ... -1.58003337e-01\n",
      "   -7.84037030e-01 -1.17809700e+00]\n",
      "  ...\n",
      "  [ 2.22474176e+02  2.22474176e+02  2.22474176e+02 ...  2.22474176e+02\n",
      "    2.22474176e+02  2.22474176e+02]\n",
      "  [ 3.32924076e+02  3.32924076e+02  3.32924076e+02 ...  3.32924076e+02\n",
      "    3.32924076e+02  3.32924076e+02]\n",
      "  [ 3.25739195e+02  3.25739195e+02  3.25739195e+02 ...  3.25739195e+02\n",
      "    3.25739195e+02  3.25739195e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.07408275e+01 -2.09021243e+01 -1.97168390e+01 ... -4.72394034e+00\n",
      "   -1.00307593e+01 -1.46406110e+01]\n",
      "  [ 1.07871773e+01  9.45764403e+00  8.31552635e+00 ...  1.05886919e+01\n",
      "    1.16722240e+01  1.19547547e+01]\n",
      "  [ 7.73964087e+00  6.69745325e+00  5.86342167e+00 ...  3.57434719e+00\n",
      "    5.11534105e+00  6.11314475e+00]\n",
      "  ...\n",
      "  [ 1.93898408e+02  1.93898408e+02  1.93898408e+02 ...  1.93898408e+02\n",
      "    1.93898408e+02  1.93898408e+02]\n",
      "  [ 3.62922007e+02  3.62922007e+02  3.62922007e+02 ...  3.62922007e+02\n",
      "    3.62922007e+02  3.62922007e+02]\n",
      "  [ 3.87272353e+02  3.87272353e+02  3.87272353e+02 ...  3.87272353e+02\n",
      "    3.87272353e+02  3.87272353e+02]]\n",
      "\n",
      " [[-1.28739509e+01 -1.33402629e+01 -1.29871391e+01 ... -2.98799166e+00\n",
      "   -6.31792698e+00 -8.42909099e+00]\n",
      "  [-1.06409276e+01 -8.81700980e+00 -7.02750342e+00 ... -8.11231115e+00\n",
      "   -9.27487174e+00 -9.11097310e+00]\n",
      "  [-4.40589134e+00 -4.03574491e+00 -3.42930058e+00 ... -1.18276493e+00\n",
      "   -2.14412190e+00 -2.70831465e+00]\n",
      "  ...\n",
      "  [ 1.93898408e+02  1.93898408e+02  1.93898408e+02 ...  1.93898408e+02\n",
      "    1.93898408e+02  1.93898408e+02]\n",
      "  [ 3.62922007e+02  3.62922007e+02  3.62922007e+02 ...  3.62922007e+02\n",
      "    3.62922007e+02  3.62922007e+02]\n",
      "  [ 3.87272353e+02  3.87272353e+02  3.87272353e+02 ...  3.87272353e+02\n",
      "    3.87272353e+02  3.87272353e+02]]\n",
      "\n",
      " [[-1.72287511e+01 -1.67588093e+01 -1.57690390e+01 ... -2.29825144e+00\n",
      "   -5.23610949e+00 -7.48081982e+00]\n",
      "  [-1.05007562e+01 -8.82734832e+00 -7.58036133e+00 ... -7.05032678e+00\n",
      "   -7.73207731e+00 -7.53453948e+00]\n",
      "  [-6.29765750e+00 -5.55503514e+00 -4.76536345e+00 ... -1.04179268e+00\n",
      "   -2.00764950e+00 -2.72531339e+00]\n",
      "  ...\n",
      "  [ 1.93898408e+02  1.93898408e+02  1.93898408e+02 ...  1.93898408e+02\n",
      "    1.93898408e+02  1.93898408e+02]\n",
      "  [ 3.62922007e+02  3.62922007e+02  3.62922007e+02 ...  3.62922007e+02\n",
      "    3.62922007e+02  3.62922007e+02]\n",
      "  [ 3.87272353e+02  3.87272353e+02  3.87272353e+02 ...  3.87272353e+02\n",
      "    3.87272353e+02  3.87272353e+02]]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Results_cross = []\n",
    "\n",
    "def findAllFile_cross(base):\n",
    "    for root, ds, fs in os.walk(base):\n",
    "        for f in fs:\n",
    "            if f.endswith('.mat'):\n",
    "                fullname = os.path.join(root, f)\n",
    "                mat = loadmat(fullname) \n",
    "                Results_cross.append(mat)\n",
    "                yield fullname\n",
    "\n",
    "#build temp list for crossing\n",
    "IKlist = []\n",
    "IAlist = []\n",
    "GVlist = []\n",
    "EPlist = []\n",
    "EP_OBSlist = []\n",
    "AP_01list = []\n",
    "AP_02list = []\n",
    "AP_03list = []\n",
    "AP_04list = []\n",
    "AP_05list = []\n",
    "AP_06list = []\n",
    "\n",
    "\n",
    "AP_01_array = np.empty((1,101))\n",
    "AP_02_array = np.empty((1,101))\n",
    "AP_03_array = np.empty((1,101))\n",
    "AP_04_array = np.empty((1,101))\n",
    "AP_05_array = np.empty((1,101))\n",
    "AP_06_array = np.empty((1,101))\n",
    "\n",
    "IK_array = np.empty((6,101,3))\n",
    "IK_tay = np.empty((3,101))\n",
    "IK_toay = np.empty((6,3,101))\n",
    "\n",
    "\n",
    "IA_array = np.empty((4,101,1))\n",
    "IA_tay = np.empty((1,101))\n",
    "IA_toay = np.empty((4,1,101))\n",
    "\n",
    "GV_array = np.empty((1,101,1))\n",
    "GV_tay = np.empty((1,101))\n",
    "GV_toay = np.empty((1,1,101))\n",
    "\n",
    "\n",
    "\n",
    "Feature = np.empty((29,101))\n",
    "\n",
    "\n",
    "\n",
    "def getfeature(value):\n",
    "    \n",
    "    \n",
    "    #EP_array = np.empty((1))\n",
    "    o = Results_cross[value]['Results']['EP']\n",
    "    EPlist.append(o)\n",
    "    \n",
    "    EP_array = np.array(EPlist[value]['LeadingToeClearance'])\n",
    "    Feature_obstacles1 = EP_array\n",
    "    \n",
    "    \n",
    "    x = Results_cross[value]['Results']['IK']\n",
    "    IKlist.append(x)\n",
    "    \n",
    "    IK_array[0,:,:] = np.array(IKlist[value]['LeadingAnkle'])\n",
    "    IK_array[1,:,:] = np.array(IKlist[value]['LeadingHip'])\n",
    "    IK_array[2,:,:] = np.array(IKlist[value]['LeadingKnee'])\n",
    "    IK_array[3,:,:] = np.array(IKlist[value]['TrailingAnkle'])\n",
    "    IK_array[4,:,:] = np.array(IKlist[value]['TrailingHip'])\n",
    "    IK_array[5,:,:] = np.array(IKlist[value]['TrailingKnee'])\n",
    "    \n",
    "    \n",
    "   \n",
    "    for i in range(IK_toay.shape[0]):  \n",
    "        IK_tay = np.transpose(IK_array[i])\n",
    "        IK_toay[i,:,:] = IK_tay\n",
    "   \n",
    "\n",
    "    k = 0\n",
    "    for i in range(IK_toay.shape[0]):\n",
    "        for j in range(IK_toay.shape[1]):\n",
    "            Feature[k,:] = IK_toay[i,j,:] #k = 3*i+j\n",
    "            k += 1\n",
    "    \n",
    "    y = Results_cross[value]['Results']['IA']\n",
    "    IAlist.append(y)\n",
    "    IA_array[0,:,:] = np.array(IAlist[value]['FrontalIA'])\n",
    "    IA_array[1,:,:] = np.array(IAlist[value]['FrontalRCIA'])\n",
    "    IA_array[2,:,:] = np.array(IAlist[value]['SagittalIA'])\n",
    "    IA_array[3,:,:] = np.array(IAlist[value]['SagittalRCIA'])\n",
    "    \n",
    "    for i in range(IA_array.shape[0]):\n",
    "      IA_tay = np.transpose(IA_array[i])\n",
    "      IA_toay[i,:,:] = IA_tay\n",
    "      \n",
    "\n",
    "    for i in range(IA_toay.shape[0]):\n",
    "        for j in range(IA_toay.shape[1]):\n",
    "            Feature[k,:] = IA_toay[i,j,:] #k = 3*i+j\n",
    "            k += 1\n",
    "    \n",
    "    z = Results_cross[value]['Results']['SelfDefinedVariables']\n",
    "    GVlist.append(z)\n",
    "    GV_array[0,:,:] = np.array(GVlist[value]['GaitVelocity'])\n",
    "    GV_tay = np.transpose(GV_array[0])\n",
    "    GV_toay[0,:,:] = GV_tay\n",
    "    Feature[22,:] = GV_toay[0,0,:]\n",
    "    '''\n",
    "    #從EP取出障礙物高度並與Clearance相加取得toe離地高度\n",
    "    A = Results_cross[value]['Results']['EP']\n",
    "    EP_OBSlist.append(A)\n",
    "    \n",
    "    EP_OBS_array = np.array(EP_OBSlist[value]['OBS']+EP_OBSlist[value]['LeadingToeClearance'])\n",
    "    Feature_obstacles1 = EP_OBS_array\n",
    "    '''\n",
    "    \n",
    "    #取出AP單一值資料並變成序列資料\n",
    "    B = Results_cross[value]['Results']['AP']['BodyHeight']\n",
    "    AP_01list.append(B)\n",
    "    for i in range(101):\n",
    "        AP_01_array[:,i] = np.array(AP_01list[value])\n",
    "        Feature[23,:] = AP_01_array[:,i]\n",
    "    \n",
    "    C = Results_cross[value]['Results']['AP']['BW']\n",
    "    AP_02list.append(C)\n",
    "    for i in range(101):\n",
    "        AP_02_array[:,i] = np.array(AP_02list[value])\n",
    "        Feature[24,:] = AP_02_array[:,i]\n",
    "    \n",
    "    D = Results_cross[value]['Results']['AP']['FootLength']\n",
    "    AP_03list.append(D)\n",
    "    for i in range(101):\n",
    "        AP_03_array[:,i] = np.array(AP_03list[value])\n",
    "        Feature[25,:] = AP_03_array[:,i]\n",
    "        \n",
    "    E = Results_cross[value]['Results']['AP']['HipWidth']\n",
    "    AP_04list.append(E)\n",
    "    for i in range(101):\n",
    "        AP_04_array[:,i] = np.array(AP_04list[value])\n",
    "        Feature[26,:] = AP_04_array[:,i]\n",
    "    \n",
    "    F = Results_cross[value]['Results']['AP']['ShankLength']\n",
    "    AP_05list.append(F)\n",
    "    for i in range(101):\n",
    "        AP_05_array[:,i] = np.array(AP_05list[value])\n",
    "        Feature[27,:] = AP_05_array[:,i]\n",
    "    \n",
    "    G = Results_cross[value]['Results']['AP']['ThighLength']\n",
    "    AP_06list.append(G)\n",
    "    for i in range(101):\n",
    "        AP_06_array[:,i] = np.array(AP_06list[value])\n",
    "        Feature[28,:] = AP_06_array[:,i]\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    return Feature,Feature_obstacles1\n",
    "\n",
    "Feature_cross = np.empty((204,29,101))\n",
    "Feature_obstacles = np.empty((204))\n",
    "FEATUREccc = np.empty((29,101))\n",
    "\n",
    "\n",
    "\n",
    "base = r\"C:/Users/Danny Lee/Desktop/motion hw/final/Results_cross/\"\n",
    "\n",
    "\n",
    "for i in findAllFile_cross(base):\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(Results_cross)):\n",
    "    FEATUREccc,Feature_obs= getfeature(i)\n",
    "    Feature_cross[i,:,:] = FEATUREccc\n",
    "    Feature_obstacles[i] = Feature_obs\n",
    "\n",
    "    \n",
    "print(Feature_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9309940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 122\n",
      "41 41\n",
      "41 41\n"
     ]
    }
   ],
   "source": [
    "X_normalized = (Feature_cross - Feature_cross.mean()) / Feature_cross.std()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, Feature_obstacles, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "print(len(X_train),len(y_train))\n",
    "print(len(X_val),len(y_val))\n",
    "print(len(X_test),len(y_test))\n",
    "#train=0.8，val=0.1，test=0.1\n",
    "\n",
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[2] , X_train.shape[1])\n",
    "X_val =X_val.reshape(X_val.shape[0],X_val.shape[2] , X_val.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[2] , X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebccfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(keras.layers.Layer):\n",
    "        def __init__(self, attention_units):\n",
    "            super(SelfAttention, self).__init__()\n",
    "            self.attention_units = attention_units\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.W = self.add_weight(\n",
    "                shape=(input_shape[-1], self.attention_units),\n",
    "                initializer=\"glorot_uniform\",\n",
    "                trainable=True\n",
    "            )\n",
    "            self.b = self.add_weight(\n",
    "                shape=(self.attention_units,),\n",
    "                initializer=\"zeros\",\n",
    "                trainable=True\n",
    "            )\n",
    "            self.V = self.add_weight(\n",
    "                shape=(self.attention_units, 1),\n",
    "                initializer=\"glorot_uniform\",\n",
    "                trainable=True\n",
    "            )\n",
    "    \n",
    "        def call(self, inputs):\n",
    "            score = keras.activations.tanh(keras.backend.dot(inputs, self.W) + self.b)\n",
    "            attention_weights = keras.activations.softmax(keras.backend.dot(score, self.V), axis=1)\n",
    "            attended_input = inputs * attention_weights\n",
    "            return attended_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366d3ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 122 samples, validate on 41 samples\n",
      "Epoch 1/2000\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 32318.3594 - val_loss: 36039.2223\n",
      "Epoch 2/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 32265.8603 - val_loss: 35959.0597\n",
      "Epoch 3/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 32174.0144 - val_loss: 35830.7167\n",
      "Epoch 4/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 32030.9151 - val_loss: 35631.4213\n",
      "Epoch 5/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 31801.4230 - val_loss: 35331.5896\n",
      "Epoch 6/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 31454.1016 - val_loss: 34847.3376\n",
      "Epoch 7/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 30872.2252 - val_loss: 33953.1159\n",
      "Epoch 8/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 29798.6786 - val_loss: 32425.6631\n",
      "Epoch 9/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 28176.3802 - val_loss: 30375.0672\n",
      "Epoch 10/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 26009.4207 - val_loss: 27583.3271\n",
      "Epoch 11/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 23002.4490 - val_loss: 23885.8131\n",
      "Epoch 12/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 19258.4315 - val_loss: 19145.2478\n",
      "Epoch 13/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 14465.4569 - val_loss: 13620.8129\n",
      "Epoch 14/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 9427.2613 - val_loss: 8001.3129\n",
      "Epoch 15/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 4616.4927 - val_loss: 4202.1993\n",
      "Epoch 16/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2867.1138 - val_loss: 4060.6285\n",
      "Epoch 17/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 4000.8062 - val_loss: 4687.4719\n",
      "Epoch 18/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 4022.4125 - val_loss: 3916.4399\n",
      "Epoch 19/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 3012.0775 - val_loss: 3730.4864\n",
      "Epoch 20/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2834.7118 - val_loss: 4140.7414\n",
      "Epoch 21/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 3027.9482 - val_loss: 4336.3009\n",
      "Epoch 22/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 3040.6655 - val_loss: 4104.7168\n",
      "Epoch 23/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2838.2275 - val_loss: 3822.7493\n",
      "Epoch 24/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2725.7836 - val_loss: 3694.3275\n",
      "Epoch 25/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2795.6746 - val_loss: 3680.6510\n",
      "Epoch 26/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2830.9871 - val_loss: 3681.3833\n",
      "Epoch 27/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2817.9954 - val_loss: 3716.0370\n",
      "Epoch 28/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2789.0797 - val_loss: 3793.3048\n",
      "Epoch 29/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2782.3003 - val_loss: 3810.0127\n",
      "Epoch 30/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2764.2205 - val_loss: 3790.2290\n",
      "Epoch 31/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2779.7114 - val_loss: 3741.2114\n",
      "Epoch 32/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2780.9047 - val_loss: 3736.1954\n",
      "Epoch 33/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2784.5564 - val_loss: 3717.1403\n",
      "Epoch 34/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2763.8048 - val_loss: 3739.8997\n",
      "Epoch 35/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2804.4038 - val_loss: 3740.1627\n",
      "Epoch 36/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2727.8747 - val_loss: 3771.5441\n",
      "Epoch 37/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2768.0572 - val_loss: 3794.0363\n",
      "Epoch 38/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2766.2107 - val_loss: 3760.1473\n",
      "Epoch 39/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2772.9969 - val_loss: 3729.1513\n",
      "Epoch 40/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2767.5148 - val_loss: 3715.6500\n",
      "Epoch 41/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2784.9636 - val_loss: 3763.6930\n",
      "Epoch 42/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2754.7876 - val_loss: 3761.4472\n",
      "Epoch 43/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2799.0730 - val_loss: 3750.5142\n",
      "Epoch 44/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2754.7489 - val_loss: 3759.7753\n",
      "Epoch 45/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2758.4348 - val_loss: 3747.7176\n",
      "Epoch 46/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2787.3122 - val_loss: 3742.7251\n",
      "Epoch 47/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2750.5430 - val_loss: 3758.4615\n",
      "Epoch 48/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2797.4188 - val_loss: 3766.4229\n",
      "Epoch 49/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2761.6979 - val_loss: 3756.6776\n",
      "Epoch 50/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2741.1508 - val_loss: 3709.1696\n",
      "Epoch 51/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2789.5984 - val_loss: 3730.7524\n",
      "Epoch 52/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2768.4908 - val_loss: 3760.8027\n",
      "Epoch 53/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2793.0373 - val_loss: 3764.8472\n",
      "Epoch 54/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2763.6554 - val_loss: 3750.9669\n",
      "Epoch 55/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2772.9974 - val_loss: 3747.4997\n",
      "Epoch 56/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2774.3790 - val_loss: 3725.8780\n",
      "Epoch 57/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2779.6860 - val_loss: 3733.9824\n",
      "Epoch 58/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2793.1711 - val_loss: 3744.1471\n",
      "Epoch 59/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2778.4593 - val_loss: 3766.5948\n",
      "Epoch 60/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2760.9643 - val_loss: 3767.2342\n",
      "Epoch 61/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2754.6667 - val_loss: 3753.4263\n",
      "Epoch 62/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2766.2447 - val_loss: 3738.8467\n",
      "Epoch 63/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2796.3740 - val_loss: 3736.6856\n",
      "Epoch 64/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2769.8374 - val_loss: 3760.4846\n",
      "Epoch 65/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2794.8775 - val_loss: 3722.1041\n",
      "Epoch 66/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2751.6734 - val_loss: 3732.1074\n",
      "Epoch 67/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2780.2287 - val_loss: 3802.8041\n",
      "Epoch 68/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2797.4979 - val_loss: 3757.6273\n",
      "Epoch 69/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2770.5752 - val_loss: 3743.1243\n",
      "Epoch 70/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2785.4956 - val_loss: 3739.6697\n",
      "Epoch 71/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2755.3956 - val_loss: 3752.1184\n",
      "Epoch 72/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2750.0430 - val_loss: 3760.0568\n",
      "Epoch 73/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2794.6755 - val_loss: 3779.9043\n",
      "Epoch 74/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2769.3568 - val_loss: 3739.5760\n",
      "Epoch 75/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 2782.2285 - val_loss: 3730.9327\n",
      "Epoch 76/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2778.6336 - val_loss: 3733.5254\n",
      "Epoch 77/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2814.2559 - val_loss: 3756.3400\n",
      "Epoch 78/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2770.3062 - val_loss: 3714.4404\n",
      "Epoch 79/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2744.1024 - val_loss: 3714.3409\n",
      "Epoch 80/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2788.8803 - val_loss: 3727.7648\n",
      "Epoch 81/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2765.6407 - val_loss: 3778.8409\n",
      "Epoch 82/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2756.9597 - val_loss: 3796.8465\n",
      "Epoch 83/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2785.6536 - val_loss: 3800.1963\n",
      "Epoch 84/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2790.0154 - val_loss: 3765.5816\n",
      "Epoch 85/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2773.3112 - val_loss: 3740.0061\n",
      "Epoch 86/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2770.3083 - val_loss: 3708.5766\n",
      "Epoch 87/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2772.9074 - val_loss: 3742.9779\n",
      "Epoch 88/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2766.7307 - val_loss: 3743.5058\n",
      "Epoch 89/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2823.8363 - val_loss: 3775.2701\n",
      "Epoch 90/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2755.2616 - val_loss: 3759.2813\n",
      "Epoch 91/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2760.2045 - val_loss: 3722.2577\n",
      "Epoch 92/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2766.1643 - val_loss: 3720.0315\n",
      "Epoch 93/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2773.3332 - val_loss: 3716.0698\n",
      "Epoch 94/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2772.7560 - val_loss: 3739.8268\n",
      "Epoch 95/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2769.2443 - val_loss: 3794.0782\n",
      "Epoch 96/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2784.3244 - val_loss: 3777.9446\n",
      "Epoch 97/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2788.6332 - val_loss: 3731.3257\n",
      "Epoch 98/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2797.1728 - val_loss: 3755.5628\n",
      "Epoch 99/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2752.7730 - val_loss: 3719.1191\n",
      "Epoch 100/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2799.4081 - val_loss: 3737.2513\n",
      "Epoch 101/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2733.7847 - val_loss: 3755.8359\n",
      "Epoch 102/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2756.0951 - val_loss: 3750.0746\n",
      "Epoch 103/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2782.7228 - val_loss: 3740.0940\n",
      "Epoch 104/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2769.4628 - val_loss: 3733.1009\n",
      "Epoch 105/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2768.5573 - val_loss: 3727.6445\n",
      "Epoch 106/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2774.5427 - val_loss: 3736.7460\n",
      "Epoch 107/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2732.3197 - val_loss: 3772.8178\n",
      "Epoch 108/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2770.7306 - val_loss: 3790.9285\n",
      "Epoch 109/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2761.4387 - val_loss: 3763.3797\n",
      "Epoch 110/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2744.2254 - val_loss: 3735.0326\n",
      "Epoch 111/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2781.5004 - val_loss: 3737.3919\n",
      "Epoch 112/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2756.1679 - val_loss: 3737.3805\n",
      "Epoch 113/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2792.7892 - val_loss: 3724.0912\n",
      "Epoch 114/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2757.2339 - val_loss: 3779.8351\n",
      "Epoch 115/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2786.0988 - val_loss: 3776.7379\n",
      "Epoch 116/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2800.6976 - val_loss: 3745.2576\n",
      "Epoch 117/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2770.7327 - val_loss: 3747.3674\n",
      "Epoch 118/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2778.3855 - val_loss: 3762.7157\n",
      "Epoch 119/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2758.8267 - val_loss: 3740.9398\n",
      "Epoch 120/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2779.9374 - val_loss: 3721.4959\n",
      "Epoch 121/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2758.3069 - val_loss: 3735.8678\n",
      "Epoch 122/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2746.3152 - val_loss: 3754.8062\n",
      "Epoch 123/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2772.1371 - val_loss: 3742.3424\n",
      "Epoch 124/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2764.2662 - val_loss: 3748.1267\n",
      "Epoch 125/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2754.5768 - val_loss: 3756.0256\n",
      "Epoch 126/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2758.0039 - val_loss: 3742.4166\n",
      "Epoch 127/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2788.5938 - val_loss: 3771.4458\n",
      "Epoch 128/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2765.2829 - val_loss: 3733.3071\n",
      "Epoch 129/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2780.1898 - val_loss: 3735.0628\n",
      "Epoch 130/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2788.6384 - val_loss: 3729.4448\n",
      "Epoch 131/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2777.8427 - val_loss: 3754.9065\n",
      "Epoch 132/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2754.6405 - val_loss: 3783.1634\n",
      "Epoch 133/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2796.6319 - val_loss: 3763.7717\n",
      "Epoch 134/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2783.3562 - val_loss: 3750.7276\n",
      "Epoch 135/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2775.9721 - val_loss: 3729.0005\n",
      "Epoch 136/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2756.9475 - val_loss: 3746.5958\n",
      "Epoch 137/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2785.1873 - val_loss: 3797.4783\n",
      "Epoch 138/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2783.1892 - val_loss: 3771.6006\n",
      "Epoch 139/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2778.3630 - val_loss: 3714.5976\n",
      "Epoch 140/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2796.9821 - val_loss: 3695.5381\n",
      "Epoch 141/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2751.3794 - val_loss: 3770.8952\n",
      "Epoch 142/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2754.9184 - val_loss: 3841.1178\n",
      "Epoch 143/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2795.2050 - val_loss: 3787.7587\n",
      "Epoch 144/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2787.6166 - val_loss: 3704.8246\n",
      "Epoch 145/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2772.0097 - val_loss: 3705.4109\n",
      "Epoch 146/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2778.7985 - val_loss: 3727.5716\n",
      "Epoch 147/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2761.3719 - val_loss: 3794.9857\n",
      "Epoch 148/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2765.2131 - val_loss: 3777.0455\n",
      "Epoch 149/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2743.4141 - val_loss: 3785.6124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2768.1746 - val_loss: 3735.8158\n",
      "Epoch 151/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2780.9423 - val_loss: 3708.7506\n",
      "Epoch 152/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2756.7612 - val_loss: 3730.3344\n",
      "Epoch 153/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2812.7901 - val_loss: 3814.4930\n",
      "Epoch 154/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2831.7199 - val_loss: 3742.1944\n",
      "Epoch 155/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2763.6453 - val_loss: 3722.3998\n",
      "Epoch 156/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2765.3283 - val_loss: 3730.3789\n",
      "Epoch 157/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2758.2337 - val_loss: 3778.0118\n",
      "Epoch 158/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2765.3178 - val_loss: 3798.0491\n",
      "Epoch 159/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2768.0277 - val_loss: 3780.3351\n",
      "Epoch 160/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2792.9013 - val_loss: 3732.0480\n",
      "Epoch 161/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2782.9456 - val_loss: 3724.2742\n",
      "Epoch 162/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2765.2992 - val_loss: 3758.3277\n",
      "Epoch 163/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2791.1715 - val_loss: 3804.9950\n",
      "Epoch 164/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2771.3789 - val_loss: 3799.9528\n",
      "Epoch 165/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2778.1471 - val_loss: 3724.9298\n",
      "Epoch 166/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2783.4278 - val_loss: 3713.3253\n",
      "Epoch 167/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2807.8599 - val_loss: 3734.3976\n",
      "Epoch 168/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2793.6562 - val_loss: 3774.9685\n",
      "Epoch 169/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2789.4125 - val_loss: 3794.1374\n",
      "Epoch 170/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2771.6356 - val_loss: 3762.0065\n",
      "Epoch 171/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2762.7935 - val_loss: 3718.4944\n",
      "Epoch 172/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2790.6725 - val_loss: 3709.8757\n",
      "Epoch 173/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2782.8611 - val_loss: 3743.0670\n",
      "Epoch 174/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2760.7376 - val_loss: 3751.3169\n",
      "Epoch 175/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2790.4247 - val_loss: 3786.8091\n",
      "Epoch 176/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2762.7881 - val_loss: 3734.5995\n",
      "Epoch 177/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2760.6661 - val_loss: 3731.9839\n",
      "Epoch 178/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2762.1272 - val_loss: 3735.2276\n",
      "Epoch 179/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2757.1203 - val_loss: 3736.5192\n",
      "Epoch 180/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2757.1035 - val_loss: 3769.4355\n",
      "Epoch 181/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2801.8308 - val_loss: 3799.8160\n",
      "Epoch 182/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2774.7122 - val_loss: 3732.0956\n",
      "Epoch 183/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2763.9805 - val_loss: 3711.0569\n",
      "Epoch 184/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2768.3693 - val_loss: 3745.1577\n",
      "Epoch 185/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2756.9726 - val_loss: 3774.5007\n",
      "Epoch 186/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2779.2590 - val_loss: 3788.2526\n",
      "Epoch 187/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2761.8379 - val_loss: 3737.2186\n",
      "Epoch 188/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2779.8300 - val_loss: 3743.4835\n",
      "Epoch 189/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2766.8437 - val_loss: 3713.4964\n",
      "Epoch 190/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2755.6879 - val_loss: 3716.3803\n",
      "Epoch 191/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2805.2564 - val_loss: 3756.4344\n",
      "Epoch 192/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2749.0004 - val_loss: 3797.3035\n",
      "Epoch 193/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2765.4202 - val_loss: 3798.1211\n",
      "Epoch 194/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2770.0663 - val_loss: 3762.7927\n",
      "Epoch 195/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2766.1159 - val_loss: 3701.4636\n",
      "Epoch 196/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2778.1448 - val_loss: 3732.1360\n",
      "Epoch 197/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2757.2719 - val_loss: 3778.6832\n",
      "Epoch 198/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2830.2218 - val_loss: 3896.7686\n",
      "Epoch 199/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2782.0137 - val_loss: 3736.1814\n",
      "Epoch 200/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2791.2087 - val_loss: 3691.5716\n",
      "Epoch 201/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2755.6562 - val_loss: 3724.4530\n",
      "Epoch 202/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2738.1038 - val_loss: 3784.4567\n",
      "Epoch 203/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2779.2692 - val_loss: 3870.8013\n",
      "Epoch 204/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2835.1153 - val_loss: 3755.5930\n",
      "Epoch 205/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2768.1287 - val_loss: 3782.8082\n",
      "Epoch 206/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2796.8653 - val_loss: 3791.1064\n",
      "Epoch 207/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2752.7518 - val_loss: 3709.2862\n",
      "Epoch 208/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2777.6930 - val_loss: 3711.1787\n",
      "Epoch 209/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2767.9142 - val_loss: 3803.3344\n",
      "Epoch 210/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2756.1830 - val_loss: 3809.1673\n",
      "Epoch 211/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2772.1976 - val_loss: 3738.8231\n",
      "Epoch 212/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2759.3789 - val_loss: 3759.0646\n",
      "Epoch 213/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2730.1452 - val_loss: 3821.5046\n",
      "Epoch 214/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2728.0890 - val_loss: 3814.3989\n",
      "Epoch 215/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2801.9726 - val_loss: 3785.1992\n",
      "Epoch 216/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2744.6476 - val_loss: 3708.4701\n",
      "Epoch 217/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2724.0678 - val_loss: 3799.2091\n",
      "Epoch 218/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2736.5751 - val_loss: 4122.0812\n",
      "Epoch 219/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2714.3902 - val_loss: 3768.9520\n",
      "Epoch 220/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2722.5298 - val_loss: 3735.6124\n",
      "Epoch 221/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2659.8904 - val_loss: 3862.7384\n",
      "Epoch 222/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2658.3425 - val_loss: 3877.4257\n",
      "Epoch 223/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2634.7681 - val_loss: 3840.4186\n",
      "Epoch 224/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 2601.3284 - val_loss: 3870.6901\n",
      "Epoch 225/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2705.2372 - val_loss: 4011.5036\n",
      "Epoch 226/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2671.7569 - val_loss: 3786.7363\n",
      "Epoch 227/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2670.5075 - val_loss: 3846.4726\n",
      "Epoch 228/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2593.4849 - val_loss: 4308.6563\n",
      "Epoch 229/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2702.6853 - val_loss: 3866.7549\n",
      "Epoch 230/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2570.3871 - val_loss: 3683.6279\n",
      "Epoch 231/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2527.3438 - val_loss: 3782.5279\n",
      "Epoch 232/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2542.3736 - val_loss: 3990.2237\n",
      "Epoch 233/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2518.9469 - val_loss: 3852.5804\n",
      "Epoch 234/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2453.8572 - val_loss: 3752.4382\n",
      "Epoch 235/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2491.1369 - val_loss: 3683.8962\n",
      "Epoch 236/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2472.2149 - val_loss: 3779.0150\n",
      "Epoch 237/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2433.6904 - val_loss: 3836.9105\n",
      "Epoch 238/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2415.6725 - val_loss: 3676.7594\n",
      "Epoch 239/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2398.1515 - val_loss: 3648.7476\n",
      "Epoch 240/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2418.2764 - val_loss: 3899.7159\n",
      "Epoch 241/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2430.0863 - val_loss: 3801.9113\n",
      "Epoch 242/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2338.4254 - val_loss: 3806.1829\n",
      "Epoch 243/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2414.2610 - val_loss: 3839.9646\n",
      "Epoch 244/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2348.7560 - val_loss: 3872.0654\n",
      "Epoch 245/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2241.0153 - val_loss: 3986.0620\n",
      "Epoch 246/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2382.3865 - val_loss: 3854.6685\n",
      "Epoch 247/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2277.1323 - val_loss: 3571.1027\n",
      "Epoch 248/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2294.1792 - val_loss: 3892.5326\n",
      "Epoch 249/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2266.9189 - val_loss: 4143.7109\n",
      "Epoch 250/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2188.2781 - val_loss: 3706.0700\n",
      "Epoch 251/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2167.7593 - val_loss: 4281.6688\n",
      "Epoch 252/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2408.4584 - val_loss: 4115.7500\n",
      "Epoch 253/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2373.3976 - val_loss: 3875.5152\n",
      "Epoch 254/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2414.7856 - val_loss: 3960.3393\n",
      "Epoch 255/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2393.1621 - val_loss: 4000.4937\n",
      "Epoch 256/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2411.9157 - val_loss: 3837.2162\n",
      "Epoch 257/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2361.8712 - val_loss: 3856.7351\n",
      "Epoch 258/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2279.4012 - val_loss: 3878.0714\n",
      "Epoch 259/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2105.3595 - val_loss: 4238.0631\n",
      "Epoch 260/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2071.7028 - val_loss: 3276.7591\n",
      "Epoch 261/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2314.9916 - val_loss: 4683.3762\n",
      "Epoch 262/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2571.5796 - val_loss: 4444.8779\n",
      "Epoch 263/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2567.8564 - val_loss: 4014.9985\n",
      "Epoch 264/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2548.4781 - val_loss: 4009.9919\n",
      "Epoch 265/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2499.6670 - val_loss: 4122.6584\n",
      "Epoch 266/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2495.2169 - val_loss: 3986.1911\n",
      "Epoch 267/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2467.0685 - val_loss: 3904.8559\n",
      "Epoch 268/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2444.1759 - val_loss: 3913.0351\n",
      "Epoch 269/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2445.1148 - val_loss: 4064.9080\n",
      "Epoch 270/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2447.3649 - val_loss: 3987.5873\n",
      "Epoch 271/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2352.9526 - val_loss: 4005.9214\n",
      "Epoch 272/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2356.7829 - val_loss: 4019.8024\n",
      "Epoch 273/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2372.7767 - val_loss: 3957.9495\n",
      "Epoch 274/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2382.2370 - val_loss: 3922.0988\n",
      "Epoch 275/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2385.6469 - val_loss: 4084.8809\n",
      "Epoch 276/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2332.2636 - val_loss: 3980.7222\n",
      "Epoch 277/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2310.6032 - val_loss: 3946.3982\n",
      "Epoch 278/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2349.9156 - val_loss: 3938.3690\n",
      "Epoch 279/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2291.6773 - val_loss: 4097.7870\n",
      "Epoch 280/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2277.9649 - val_loss: 3945.8134\n",
      "Epoch 281/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2293.6099 - val_loss: 3884.0951\n",
      "Epoch 282/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2371.6506 - val_loss: 4144.7122\n",
      "Epoch 283/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2265.6116 - val_loss: 3843.6630\n",
      "Epoch 284/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2259.7309 - val_loss: 3913.4766\n",
      "Epoch 285/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2206.6657 - val_loss: 3957.7724\n",
      "Epoch 286/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2205.2863 - val_loss: 3855.3903\n",
      "Epoch 287/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2224.4794 - val_loss: 3972.4412\n",
      "Epoch 288/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2237.0307 - val_loss: 3881.7558\n",
      "Epoch 289/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2222.1943 - val_loss: 4067.2216\n",
      "Epoch 290/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2181.2244 - val_loss: 3888.8898\n",
      "Epoch 291/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2164.7921 - val_loss: 3904.1450\n",
      "Epoch 292/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2195.0358 - val_loss: 4184.9767\n",
      "Epoch 293/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2115.6020 - val_loss: 3852.7698\n",
      "Epoch 294/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2136.5366 - val_loss: 3853.9058\n",
      "Epoch 295/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2098.5303 - val_loss: 4223.1598\n",
      "Epoch 296/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2078.1153 - val_loss: 3986.6254\n",
      "Epoch 297/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2024.5719 - val_loss: 3798.3103\n",
      "Epoch 298/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 2101.9312 - val_loss: 3904.9788\n",
      "Epoch 299/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2059.3412 - val_loss: 4120.4762\n",
      "Epoch 300/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2007.7342 - val_loss: 3687.4960\n",
      "Epoch 301/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2058.1282 - val_loss: 3706.3427\n",
      "Epoch 302/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1960.0936 - val_loss: 4103.6398\n",
      "Epoch 303/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2006.8895 - val_loss: 3698.4652\n",
      "Epoch 304/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1945.9454 - val_loss: 3888.1016\n",
      "Epoch 305/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1893.1187 - val_loss: 3621.6942\n",
      "Epoch 306/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1900.4454 - val_loss: 3653.6280\n",
      "Epoch 307/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1926.5008 - val_loss: 3200.6316\n",
      "Epoch 308/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2079.8398 - val_loss: 3381.0668\n",
      "Epoch 309/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1895.7662 - val_loss: 3899.4772\n",
      "Epoch 310/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1922.2624 - val_loss: 3106.7128\n",
      "Epoch 311/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1869.3552 - val_loss: 3274.1637\n",
      "Epoch 312/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1841.4970 - val_loss: 2869.0486\n",
      "Epoch 313/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1977.2320 - val_loss: 2878.2014\n",
      "Epoch 314/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1813.5374 - val_loss: 3001.3808\n",
      "Epoch 315/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1830.2828 - val_loss: 2781.4904\n",
      "Epoch 316/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1925.1103 - val_loss: 2850.2724\n",
      "Epoch 317/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1891.8614 - val_loss: 2659.7382\n",
      "Epoch 318/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1764.2454 - val_loss: 3059.2083\n",
      "Epoch 319/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1793.3302 - val_loss: 2889.4308\n",
      "Epoch 320/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1848.2932 - val_loss: 2567.8740\n",
      "Epoch 321/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2052.9626 - val_loss: 2966.3589\n",
      "Epoch 322/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1862.9435 - val_loss: 2970.3359\n",
      "Epoch 323/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1819.4805 - val_loss: 2810.8920\n",
      "Epoch 324/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1736.7736 - val_loss: 3124.9542\n",
      "Epoch 325/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1685.7172 - val_loss: 2721.2356\n",
      "Epoch 326/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1793.9993 - val_loss: 3270.6432\n",
      "Epoch 327/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1705.5141 - val_loss: 2619.1659\n",
      "Epoch 328/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1682.8327 - val_loss: 2733.0846\n",
      "Epoch 329/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1652.9918 - val_loss: 2884.0982\n",
      "Epoch 330/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1714.5192 - val_loss: 2827.7485\n",
      "Epoch 331/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1579.4225 - val_loss: 2792.7201\n",
      "Epoch 332/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1594.5744 - val_loss: 2584.7550\n",
      "Epoch 333/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1480.9527 - val_loss: 2874.4865\n",
      "Epoch 334/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1622.4380 - val_loss: 2375.0111\n",
      "Epoch 335/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1942.8272 - val_loss: 2602.8065\n",
      "Epoch 336/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1731.1248 - val_loss: 3298.3742\n",
      "Epoch 337/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1657.4187 - val_loss: 2421.3412\n",
      "Epoch 338/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1603.6097 - val_loss: 4064.4745\n",
      "Epoch 339/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2360.4919 - val_loss: 2478.7108\n",
      "Epoch 340/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1799.6526 - val_loss: 2988.0581\n",
      "Epoch 341/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2087.6690 - val_loss: 3198.8456\n",
      "Epoch 342/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1920.9152 - val_loss: 2972.8224\n",
      "Epoch 343/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1681.6486 - val_loss: 2599.9710\n",
      "Epoch 344/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1659.8396 - val_loss: 2684.4204\n",
      "Epoch 345/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1617.6859 - val_loss: 2697.0219\n",
      "Epoch 346/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1535.7450 - val_loss: 3183.4745\n",
      "Epoch 347/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1482.6826 - val_loss: 2256.8869\n",
      "Epoch 348/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1510.8905 - val_loss: 2690.8677\n",
      "Epoch 349/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1510.1014 - val_loss: 2476.2421\n",
      "Epoch 350/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1424.6996 - val_loss: 2408.4954\n",
      "Epoch 351/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1417.2472 - val_loss: 2418.1119\n",
      "Epoch 352/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1465.9465 - val_loss: 2452.4499\n",
      "Epoch 353/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1480.7101 - val_loss: 2689.1355\n",
      "Epoch 354/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1393.0115 - val_loss: 2397.2214\n",
      "Epoch 355/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1325.1152 - val_loss: 2736.6916\n",
      "Epoch 356/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1398.4767 - val_loss: 2507.2790\n",
      "Epoch 357/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1493.9122 - val_loss: 3106.6345\n",
      "Epoch 358/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1797.5349 - val_loss: 3031.1756\n",
      "Epoch 359/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1782.2706 - val_loss: 2695.9347\n",
      "Epoch 360/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1725.8035 - val_loss: 2699.5490\n",
      "Epoch 361/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1532.7021 - val_loss: 2570.2148\n",
      "Epoch 362/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1437.5580 - val_loss: 2242.6812\n",
      "Epoch 363/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1385.3210 - val_loss: 2343.5037\n",
      "Epoch 364/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1271.0137 - val_loss: 2456.6475\n",
      "Epoch 365/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1238.4766 - val_loss: 2483.8108\n",
      "Epoch 366/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1224.0815 - val_loss: 2598.9095\n",
      "Epoch 367/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1194.0747 - val_loss: 2568.5250\n",
      "Epoch 368/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1221.8865 - val_loss: 2515.7106\n",
      "Epoch 369/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1149.0233 - val_loss: 2656.8741\n",
      "Epoch 370/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1144.1444 - val_loss: 2595.0124\n",
      "Epoch 371/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1212.5434 - val_loss: 2180.2471\n",
      "Epoch 372/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 1204.9286 - val_loss: 2522.0512\n",
      "Epoch 373/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1184.6074 - val_loss: 2307.0727\n",
      "Epoch 374/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1145.5841 - val_loss: 2429.1733\n",
      "Epoch 375/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1187.6391 - val_loss: 2319.9834\n",
      "Epoch 376/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1264.0764 - val_loss: 2150.6497\n",
      "Epoch 377/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1367.8867 - val_loss: 2757.8659\n",
      "Epoch 378/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1205.8946 - val_loss: 2325.6823\n",
      "Epoch 379/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1114.9662 - val_loss: 2501.3353\n",
      "Epoch 380/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1247.5545 - val_loss: 2162.3086\n",
      "Epoch 381/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1135.1742 - val_loss: 2743.2371\n",
      "Epoch 382/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1513.0749 - val_loss: 2144.0021\n",
      "Epoch 383/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1498.2740 - val_loss: 2169.4913\n",
      "Epoch 384/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1300.2104 - val_loss: 2621.6254\n",
      "Epoch 385/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1336.9687 - val_loss: 2352.5430\n",
      "Epoch 386/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1205.5116 - val_loss: 2226.5587\n",
      "Epoch 387/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1052.3752 - val_loss: 2183.1078\n",
      "Epoch 388/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1090.0994 - val_loss: 2687.7810\n",
      "Epoch 389/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1245.8806 - val_loss: 2523.8341\n",
      "Epoch 390/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1222.6889 - val_loss: 2208.1185\n",
      "Epoch 391/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1393.2496 - val_loss: 2346.3418\n",
      "Epoch 392/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1148.9875 - val_loss: 2204.6896\n",
      "Epoch 393/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1175.9935 - val_loss: 2221.8184\n",
      "Epoch 394/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 991.3886 - val_loss: 2275.9308\n",
      "Epoch 395/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1066.0780 - val_loss: 2591.8758\n",
      "Epoch 396/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1064.5039 - val_loss: 2306.9968\n",
      "Epoch 397/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 981.9064 - val_loss: 2426.1342\n",
      "Epoch 398/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 869.8341 - val_loss: 2166.9910\n",
      "Epoch 399/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 891.7905 - val_loss: 2153.2854\n",
      "Epoch 400/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 920.3434 - val_loss: 2232.8048\n",
      "Epoch 401/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 835.5539 - val_loss: 2306.4298\n",
      "Epoch 402/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 880.1754 - val_loss: 2153.3403\n",
      "Epoch 403/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1012.4667 - val_loss: 2166.6032\n",
      "Epoch 404/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1008.2255 - val_loss: 2339.6442\n",
      "Epoch 405/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 899.3446 - val_loss: 2146.9245\n",
      "Epoch 406/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 877.7006 - val_loss: 2357.4283\n",
      "Epoch 407/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 859.1845 - val_loss: 2334.9530\n",
      "Epoch 408/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 978.2306 - val_loss: 3173.9346\n",
      "Epoch 409/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1576.7303 - val_loss: 2496.1587\n",
      "Epoch 410/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1435.2816 - val_loss: 2349.3487\n",
      "Epoch 411/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1396.2300 - val_loss: 2499.2020\n",
      "Epoch 412/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1366.9295 - val_loss: 2612.0251\n",
      "Epoch 413/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1142.3194 - val_loss: 2278.9077\n",
      "Epoch 414/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1215.1008 - val_loss: 2205.6098\n",
      "Epoch 415/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1119.1544 - val_loss: 2188.2957\n",
      "Epoch 416/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 979.9266 - val_loss: 1963.7616\n",
      "Epoch 417/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 933.2897 - val_loss: 1869.0462\n",
      "Epoch 418/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 931.1606 - val_loss: 1879.0283\n",
      "Epoch 419/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 915.8551 - val_loss: 2015.8483\n",
      "Epoch 420/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 855.7888 - val_loss: 2165.1441\n",
      "Epoch 421/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 796.9788 - val_loss: 1983.6573\n",
      "Epoch 422/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 886.4457 - val_loss: 2268.1343\n",
      "Epoch 423/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 946.8860 - val_loss: 1961.4565\n",
      "Epoch 424/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 876.4045 - val_loss: 1887.7994\n",
      "Epoch 425/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 727.8943 - val_loss: 2144.8853\n",
      "Epoch 426/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 741.5801 - val_loss: 1997.1547\n",
      "Epoch 427/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 703.7953 - val_loss: 1909.1428\n",
      "Epoch 428/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 650.8373 - val_loss: 2413.2834\n",
      "Epoch 429/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 653.2602 - val_loss: 2159.0504\n",
      "Epoch 430/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 630.8297 - val_loss: 2144.4972\n",
      "Epoch 431/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 659.3171 - val_loss: 2399.1759\n",
      "Epoch 432/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 614.1944 - val_loss: 2469.4337\n",
      "Epoch 433/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 711.6746 - val_loss: 2312.7118\n",
      "Epoch 434/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 987.6440 - val_loss: 2900.4407\n",
      "Epoch 435/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 940.0351 - val_loss: 2088.4945\n",
      "Epoch 436/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 916.6308 - val_loss: 2174.3722\n",
      "Epoch 437/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 853.1970 - val_loss: 1937.8694\n",
      "Epoch 438/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1142.9494 - val_loss: 3224.6886\n",
      "Epoch 439/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1488.6352 - val_loss: 2126.4388\n",
      "Epoch 440/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1402.3833 - val_loss: 2514.6870\n",
      "Epoch 441/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1209.1125 - val_loss: 2771.8308\n",
      "Epoch 442/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1044.4014 - val_loss: 1911.6867\n",
      "Epoch 443/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1117.7105 - val_loss: 2008.0538\n",
      "Epoch 444/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 917.0198 - val_loss: 2343.1144\n",
      "Epoch 445/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 939.5192 - val_loss: 2316.3876\n",
      "Epoch 446/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 839.1826 - val_loss: 2325.3458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 843.6144 - val_loss: 2119.6667\n",
      "Epoch 448/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 796.7146 - val_loss: 2294.2817\n",
      "Epoch 449/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 934.9387 - val_loss: 2242.7070\n",
      "Epoch 450/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 740.2280 - val_loss: 2142.3472\n",
      "Epoch 451/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 694.8848 - val_loss: 2149.0429\n",
      "Epoch 452/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 742.8383 - val_loss: 1975.7771\n",
      "Epoch 453/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 638.1727 - val_loss: 2265.9954\n",
      "Epoch 454/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 650.3542 - val_loss: 2162.6273\n",
      "Epoch 455/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 624.2515 - val_loss: 2086.9375\n",
      "Epoch 456/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 665.4421 - val_loss: 2186.8211\n",
      "Epoch 457/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1037.9712 - val_loss: 2246.5061\n",
      "Epoch 458/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 865.3003 - val_loss: 2078.7897\n",
      "Epoch 459/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2172.7417 - val_loss: 2025.8719\n",
      "Epoch 460/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1033.9291 - val_loss: 2303.4032\n",
      "Epoch 461/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 836.0124 - val_loss: 2322.5242\n",
      "Epoch 462/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 811.5325 - val_loss: 2298.7513\n",
      "Epoch 463/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 795.7689 - val_loss: 2039.1313\n",
      "Epoch 464/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 901.9629 - val_loss: 2157.4273\n",
      "Epoch 465/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 689.4812 - val_loss: 2242.1679\n",
      "Epoch 466/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 616.1140 - val_loss: 2366.2309\n",
      "Epoch 467/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1779.8797 - val_loss: 3880.1243\n",
      "Epoch 468/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2965.5903 - val_loss: 3451.2412\n",
      "Epoch 469/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2455.9127 - val_loss: 3418.2382\n",
      "Epoch 470/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2576.7739 - val_loss: 3265.4683\n",
      "Epoch 471/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2559.1128 - val_loss: 3370.6085\n",
      "Epoch 472/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2569.3219 - val_loss: 3475.1182\n",
      "Epoch 473/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2511.3045 - val_loss: 3354.2355\n",
      "Epoch 474/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2500.0741 - val_loss: 3352.4164\n",
      "Epoch 475/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2472.0703 - val_loss: 3363.0758\n",
      "Epoch 476/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2403.2624 - val_loss: 3353.6637\n",
      "Epoch 477/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2368.5761 - val_loss: 3450.5141\n",
      "Epoch 478/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2352.4941 - val_loss: 3467.9609\n",
      "Epoch 479/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2330.2597 - val_loss: 3354.2477\n",
      "Epoch 480/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2350.1129 - val_loss: 3366.0925\n",
      "Epoch 481/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2305.2456 - val_loss: 3468.0076\n",
      "Epoch 482/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2306.3730 - val_loss: 3428.3054\n",
      "Epoch 483/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2294.3617 - val_loss: 3402.4211\n",
      "Epoch 484/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2261.7899 - val_loss: 3361.9221\n",
      "Epoch 485/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2225.3713 - val_loss: 3431.8099\n",
      "Epoch 486/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2228.5382 - val_loss: 3393.0517\n",
      "Epoch 487/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2205.3214 - val_loss: 3353.8306\n",
      "Epoch 488/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2223.4541 - val_loss: 3337.9664\n",
      "Epoch 489/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2218.3434 - val_loss: 3380.4397\n",
      "Epoch 490/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2228.5660 - val_loss: 3331.2339\n",
      "Epoch 491/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2203.4095 - val_loss: 3265.4350\n",
      "Epoch 492/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2202.8494 - val_loss: 3257.8836\n",
      "Epoch 493/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2159.9068 - val_loss: 3243.2429\n",
      "Epoch 494/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2148.3961 - val_loss: 3210.1417\n",
      "Epoch 495/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2135.8445 - val_loss: 3175.2271\n",
      "Epoch 496/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2107.8325 - val_loss: 3070.3085\n",
      "Epoch 497/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2116.6170 - val_loss: 3040.9291\n",
      "Epoch 498/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2041.3878 - val_loss: 3075.6230\n",
      "Epoch 499/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1972.7581 - val_loss: 2963.4816\n",
      "Epoch 500/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1966.0016 - val_loss: 2922.7524\n",
      "Epoch 501/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1936.2413 - val_loss: 3004.9655\n",
      "Epoch 502/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1899.6030 - val_loss: 2917.3149\n",
      "Epoch 503/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1847.3335 - val_loss: 2899.1003\n",
      "Epoch 504/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1802.6249 - val_loss: 2799.6400\n",
      "Epoch 505/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1800.5096 - val_loss: 2784.7450\n",
      "Epoch 506/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1720.8505 - val_loss: 2569.5953\n",
      "Epoch 507/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1639.9447 - val_loss: 2503.9325\n",
      "Epoch 508/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1620.6933 - val_loss: 2457.1928\n",
      "Epoch 509/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1562.8984 - val_loss: 2354.1543\n",
      "Epoch 510/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1557.4082 - val_loss: 2254.6490\n",
      "Epoch 511/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1527.9951 - val_loss: 2174.9191\n",
      "Epoch 512/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1492.3410 - val_loss: 2256.3409\n",
      "Epoch 513/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1476.3546 - val_loss: 2260.7534\n",
      "Epoch 514/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1422.2384 - val_loss: 2122.3050\n",
      "Epoch 515/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1408.3057 - val_loss: 1962.8057\n",
      "Epoch 516/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1366.8426 - val_loss: 2053.1053\n",
      "Epoch 517/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1420.5611 - val_loss: 2234.0133\n",
      "Epoch 518/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1320.1517 - val_loss: 1969.4802\n",
      "Epoch 519/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1404.0040 - val_loss: 1936.2318\n",
      "Epoch 520/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1309.2328 - val_loss: 2058.4024\n",
      "Epoch 521/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 1308.8467 - val_loss: 1970.5348\n",
      "Epoch 522/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1288.9468 - val_loss: 1949.3408\n",
      "Epoch 523/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1229.8268 - val_loss: 1842.9956\n",
      "Epoch 524/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1182.0204 - val_loss: 1898.9315\n",
      "Epoch 525/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1167.0424 - val_loss: 1867.4053\n",
      "Epoch 526/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1123.4792 - val_loss: 1873.5909\n",
      "Epoch 527/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1098.5123 - val_loss: 2031.0924\n",
      "Epoch 528/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1070.5406 - val_loss: 1686.7662\n",
      "Epoch 529/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1081.2437 - val_loss: 1780.8947\n",
      "Epoch 530/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 983.7397 - val_loss: 1849.5016\n",
      "Epoch 531/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1008.6479 - val_loss: 1802.6890\n",
      "Epoch 532/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1002.1984 - val_loss: 1830.7751\n",
      "Epoch 533/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 984.6170 - val_loss: 1814.4159\n",
      "Epoch 534/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 933.1576 - val_loss: 1867.6822\n",
      "Epoch 535/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 979.4735 - val_loss: 1753.8156\n",
      "Epoch 536/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 920.5841 - val_loss: 1745.5284\n",
      "Epoch 537/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 921.3944 - val_loss: 1778.0260\n",
      "Epoch 538/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 924.1407 - val_loss: 1802.9680\n",
      "Epoch 539/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 999.0645 - val_loss: 1931.5770\n",
      "Epoch 540/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1072.0539 - val_loss: 1939.0835\n",
      "Epoch 541/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1054.5801 - val_loss: 2142.0301\n",
      "Epoch 542/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 992.0654 - val_loss: 1898.9609\n",
      "Epoch 543/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1012.7872 - val_loss: 1878.8268\n",
      "Epoch 544/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 982.3370 - val_loss: 1683.1867\n",
      "Epoch 545/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1032.6334 - val_loss: 2200.9530\n",
      "Epoch 546/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1109.1189 - val_loss: 1968.7514\n",
      "Epoch 547/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 954.7307 - val_loss: 1893.5023\n",
      "Epoch 548/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 898.6761 - val_loss: 1653.3061\n",
      "Epoch 549/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 884.5012 - val_loss: 1933.5365\n",
      "Epoch 550/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 924.9751 - val_loss: 1733.9482\n",
      "Epoch 551/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 854.0298 - val_loss: 2043.9831\n",
      "Epoch 552/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 909.2307 - val_loss: 1712.3952\n",
      "Epoch 553/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 935.5897 - val_loss: 1994.0493\n",
      "Epoch 554/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1073.7970 - val_loss: 2062.8357\n",
      "Epoch 555/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 963.9448 - val_loss: 1698.8205\n",
      "Epoch 556/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 931.0991 - val_loss: 1797.9088\n",
      "Epoch 557/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1084.9757 - val_loss: 1966.9047\n",
      "Epoch 558/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 946.1113 - val_loss: 1740.7994\n",
      "Epoch 559/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 977.7613 - val_loss: 1708.5372\n",
      "Epoch 560/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 970.9564 - val_loss: 1800.2815\n",
      "Epoch 561/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 841.9632 - val_loss: 1753.6824\n",
      "Epoch 562/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 832.4440 - val_loss: 1668.5754\n",
      "Epoch 563/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 884.2911 - val_loss: 1794.8362\n",
      "Epoch 564/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 835.5107 - val_loss: 1918.5041\n",
      "Epoch 565/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 769.1247 - val_loss: 1830.6630\n",
      "Epoch 566/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 832.5897 - val_loss: 2049.2873\n",
      "Epoch 567/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 953.3233 - val_loss: 2027.3653\n",
      "Epoch 568/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 882.8327 - val_loss: 1450.0372\n",
      "Epoch 569/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 989.3453 - val_loss: 1739.4787\n",
      "Epoch 570/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1235.8041 - val_loss: 2470.1406\n",
      "Epoch 571/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1071.1094 - val_loss: 1743.0024\n",
      "Epoch 572/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1053.6592 - val_loss: 1599.2100\n",
      "Epoch 573/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 976.2339 - val_loss: 1866.7326\n",
      "Epoch 574/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1058.0924 - val_loss: 1722.1541\n",
      "Epoch 575/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 846.1402 - val_loss: 1838.1255\n",
      "Epoch 576/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 849.0567 - val_loss: 1838.4518\n",
      "Epoch 577/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 848.0859 - val_loss: 1830.2212\n",
      "Epoch 578/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 793.0820 - val_loss: 1627.4885\n",
      "Epoch 579/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 791.6697 - val_loss: 1412.3298\n",
      "Epoch 580/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 769.4380 - val_loss: 1715.1740\n",
      "Epoch 581/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 782.3978 - val_loss: 1548.8351\n",
      "Epoch 582/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 851.9900 - val_loss: 1931.7320\n",
      "Epoch 583/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 835.7949 - val_loss: 1993.8293\n",
      "Epoch 584/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 689.5019 - val_loss: 1830.9732\n",
      "Epoch 585/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 923.2447 - val_loss: 1612.4241\n",
      "Epoch 586/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 718.1425 - val_loss: 1814.9382\n",
      "Epoch 587/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 821.5532 - val_loss: 1846.1671\n",
      "Epoch 588/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 741.8316 - val_loss: 1741.7919\n",
      "Epoch 589/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 780.7666 - val_loss: 1704.5202\n",
      "Epoch 590/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 819.6679 - val_loss: 1850.0794\n",
      "Epoch 591/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 695.3157 - val_loss: 1808.9935\n",
      "Epoch 592/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 677.7386 - val_loss: 1595.2856\n",
      "Epoch 593/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 786.5610 - val_loss: 1941.4345\n",
      "Epoch 594/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 752.3457 - val_loss: 1982.3458\n",
      "Epoch 595/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 739.6389 - val_loss: 1656.5701\n",
      "Epoch 596/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 1009.4218 - val_loss: 2068.9239\n",
      "Epoch 597/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1320.2696 - val_loss: 2387.2337\n",
      "Epoch 598/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1414.8330 - val_loss: 2125.5386\n",
      "Epoch 599/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1456.4032 - val_loss: 2008.0436\n",
      "Epoch 600/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1087.4615 - val_loss: 2019.0386\n",
      "Epoch 601/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1052.9025 - val_loss: 2113.6999\n",
      "Epoch 602/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 982.2328 - val_loss: 2112.4014\n",
      "Epoch 603/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 883.9391 - val_loss: 2102.5916\n",
      "Epoch 604/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 740.1864 - val_loss: 1899.6537\n",
      "Epoch 605/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 701.3396 - val_loss: 1865.1332\n",
      "Epoch 606/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 936.6886 - val_loss: 2490.1446\n",
      "Epoch 607/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 880.0589 - val_loss: 1942.2333\n",
      "Epoch 608/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 805.9636 - val_loss: 1878.9999\n",
      "Epoch 609/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 712.6799 - val_loss: 1982.9399\n",
      "Epoch 610/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 701.9002 - val_loss: 2150.4641\n",
      "Epoch 611/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 712.6850 - val_loss: 2012.3840\n",
      "Epoch 612/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 648.6969 - val_loss: 2068.4073\n",
      "Epoch 613/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 638.1967 - val_loss: 2002.9674\n",
      "Epoch 614/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 645.4793 - val_loss: 2021.6441\n",
      "Epoch 615/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 619.0106 - val_loss: 2018.0929\n",
      "Epoch 616/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 702.0644 - val_loss: 1962.8775\n",
      "Epoch 617/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 702.5647 - val_loss: 1706.5276\n",
      "Epoch 618/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 658.1264 - val_loss: 2075.1200\n",
      "Epoch 619/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 623.7421 - val_loss: 2033.7896\n",
      "Epoch 620/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 656.5666 - val_loss: 1750.8401\n",
      "Epoch 621/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 749.7499 - val_loss: 1809.7831\n",
      "Epoch 622/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 723.6122 - val_loss: 2085.1999\n",
      "Epoch 623/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 648.5108 - val_loss: 1852.6103\n",
      "Epoch 624/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 616.5857 - val_loss: 2056.3866\n",
      "Epoch 625/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 760.2419 - val_loss: 2029.5042\n",
      "Epoch 626/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 605.0506 - val_loss: 1641.4930\n",
      "Epoch 627/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 665.3560 - val_loss: 1803.0929\n",
      "Epoch 628/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 680.1200 - val_loss: 2134.8839\n",
      "Epoch 629/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 704.6974 - val_loss: 1689.8848\n",
      "Epoch 630/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 608.8655 - val_loss: 1774.1000\n",
      "Epoch 631/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 591.9458 - val_loss: 1930.5207\n",
      "Epoch 632/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 585.6956 - val_loss: 1833.0959\n",
      "Epoch 633/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 642.5990 - val_loss: 2164.7439\n",
      "Epoch 634/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 603.6851 - val_loss: 2138.5569\n",
      "Epoch 635/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 556.6784 - val_loss: 1808.5282\n",
      "Epoch 636/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 566.1842 - val_loss: 1988.1355\n",
      "Epoch 637/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 518.4661 - val_loss: 2123.9828\n",
      "Epoch 638/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 552.9995 - val_loss: 2091.5624\n",
      "Epoch 639/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 520.7580 - val_loss: 1658.9974\n",
      "Epoch 640/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 586.3035 - val_loss: 1754.3883\n",
      "Epoch 641/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 587.3205 - val_loss: 2059.2027\n",
      "Epoch 642/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 587.6750 - val_loss: 1839.4641\n",
      "Epoch 643/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 598.0599 - val_loss: 2244.2801\n",
      "Epoch 644/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 652.4115 - val_loss: 2173.3632\n",
      "Epoch 645/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 621.9761 - val_loss: 1748.5104\n",
      "Epoch 646/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 645.3986 - val_loss: 1532.2564\n",
      "Epoch 647/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 572.6386 - val_loss: 1848.3412\n",
      "Epoch 648/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 547.4171 - val_loss: 1886.1110\n",
      "Epoch 649/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 554.8872 - val_loss: 1958.4124\n",
      "Epoch 650/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 581.2711 - val_loss: 2123.4331\n",
      "Epoch 651/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 510.7307 - val_loss: 1841.2925\n",
      "Epoch 652/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 554.2120 - val_loss: 1888.8447\n",
      "Epoch 653/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 669.4562 - val_loss: 2157.5978\n",
      "Epoch 654/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 480.9688 - val_loss: 1662.4708\n",
      "Epoch 655/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 591.5698 - val_loss: 1864.0798\n",
      "Epoch 656/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 569.4051 - val_loss: 2088.5332\n",
      "Epoch 657/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 537.2412 - val_loss: 1666.1700\n",
      "Epoch 658/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 675.1041 - val_loss: 1726.2749\n",
      "Epoch 659/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 649.8605 - val_loss: 1952.8647\n",
      "Epoch 660/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 797.8167 - val_loss: 1987.2464\n",
      "Epoch 661/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 680.9705 - val_loss: 1458.4527\n",
      "Epoch 662/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 716.4516 - val_loss: 1763.3447\n",
      "Epoch 663/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 628.8365 - val_loss: 2108.5254\n",
      "Epoch 664/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 706.1252 - val_loss: 1912.0282\n",
      "Epoch 665/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 532.9224 - val_loss: 1679.6700\n",
      "Epoch 666/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 540.2601 - val_loss: 2025.7053\n",
      "Epoch 667/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 584.6595 - val_loss: 2031.8131\n",
      "Epoch 668/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 541.9367 - val_loss: 1593.7458\n",
      "Epoch 669/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 495.6231 - val_loss: 1793.1721\n",
      "Epoch 670/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 490.5112 - val_loss: 1973.5268\n",
      "Epoch 671/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 519.5194 - val_loss: 1871.1716\n",
      "Epoch 672/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 475.3756 - val_loss: 1688.7909\n",
      "Epoch 673/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 493.6207 - val_loss: 1647.0366\n",
      "Epoch 674/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 472.0287 - val_loss: 1862.5577\n",
      "Epoch 675/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 451.4398 - val_loss: 1837.9446\n",
      "Epoch 676/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 470.6130 - val_loss: 1866.2825\n",
      "Epoch 677/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 441.5227 - val_loss: 1778.6718\n",
      "Epoch 678/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 449.7064 - val_loss: 1766.8700\n",
      "Epoch 679/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 450.9262 - val_loss: 1756.7423\n",
      "Epoch 680/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 442.5230 - val_loss: 1747.1110\n",
      "Epoch 681/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 491.0294 - val_loss: 1698.6176\n",
      "Epoch 682/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 562.0930 - val_loss: 1826.6852\n",
      "Epoch 683/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 426.0424 - val_loss: 1819.9254\n",
      "Epoch 684/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 505.8468 - val_loss: 2043.1222\n",
      "Epoch 685/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 555.1266 - val_loss: 1679.3915\n",
      "Epoch 686/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 700.2468 - val_loss: 2476.7280\n",
      "Epoch 687/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 848.0605 - val_loss: 2313.6899\n",
      "Epoch 688/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 829.0645 - val_loss: 2089.3156\n",
      "Epoch 689/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 626.7110 - val_loss: 1769.0393\n",
      "Epoch 690/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 631.0231 - val_loss: 1930.6060\n",
      "Epoch 691/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 481.9407 - val_loss: 2177.4119\n",
      "Epoch 692/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 458.5428 - val_loss: 1899.9393\n",
      "Epoch 693/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 491.9268 - val_loss: 1834.3408\n",
      "Epoch 694/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 474.5883 - val_loss: 2009.8220\n",
      "Epoch 695/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 465.6786 - val_loss: 1828.1769\n",
      "Epoch 696/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 494.4867 - val_loss: 1928.0132\n",
      "Epoch 697/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 581.4715 - val_loss: 2161.8010\n",
      "Epoch 698/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 554.6827 - val_loss: 1798.5451\n",
      "Epoch 699/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 582.3503 - val_loss: 1461.2629\n",
      "Epoch 700/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 464.9491 - val_loss: 2068.0674\n",
      "Epoch 701/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 553.3319 - val_loss: 1887.4113\n",
      "Epoch 702/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 505.4379 - val_loss: 1804.5682\n",
      "Epoch 703/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 502.5489 - val_loss: 1814.5706\n",
      "Epoch 704/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 472.6603 - val_loss: 1947.1176\n",
      "Epoch 705/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 514.1271 - val_loss: 1966.1994\n",
      "Epoch 706/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 441.5921 - val_loss: 1926.9678\n",
      "Epoch 707/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 494.6816 - val_loss: 1593.2869\n",
      "Epoch 708/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 474.7566 - val_loss: 1784.2447\n",
      "Epoch 709/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 449.7331 - val_loss: 2155.1360\n",
      "Epoch 710/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 464.8010 - val_loss: 1866.9611\n",
      "Epoch 711/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 447.3104 - val_loss: 1603.3317\n",
      "Epoch 712/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 459.0863 - val_loss: 2101.4669\n",
      "Epoch 713/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 413.3335 - val_loss: 1968.8092\n",
      "Epoch 714/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 386.9286 - val_loss: 1964.5515\n",
      "Epoch 715/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 381.0689 - val_loss: 1812.3232\n",
      "Epoch 716/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 354.5649 - val_loss: 1899.7231\n",
      "Epoch 717/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 352.6903 - val_loss: 1793.4228\n",
      "Epoch 718/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 332.7274 - val_loss: 2094.7355\n",
      "Epoch 719/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 378.6818 - val_loss: 1799.4586\n",
      "Epoch 720/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 401.9195 - val_loss: 1644.5076\n",
      "Epoch 721/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 381.9369 - val_loss: 1880.5797\n",
      "Epoch 722/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 381.0957 - val_loss: 1609.7820\n",
      "Epoch 723/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 422.2387 - val_loss: 2002.8225\n",
      "Epoch 724/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 523.2199 - val_loss: 1934.4995\n",
      "Epoch 725/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 556.8096 - val_loss: 1754.8112\n",
      "Epoch 726/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 573.4064 - val_loss: 2171.5942\n",
      "Epoch 727/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 533.2438 - val_loss: 1636.0580\n",
      "Epoch 728/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 536.3764 - val_loss: 2140.8372\n",
      "Epoch 729/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 745.7965 - val_loss: 2252.4386\n",
      "Epoch 730/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 610.1670 - val_loss: 1841.1798\n",
      "Epoch 731/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 605.6850 - val_loss: 1933.4621\n",
      "Epoch 732/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 545.9976 - val_loss: 2383.3641\n",
      "Epoch 733/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 522.9083 - val_loss: 1985.4955\n",
      "Epoch 734/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 488.8394 - val_loss: 1833.9950\n",
      "Epoch 735/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 451.9811 - val_loss: 2096.0950\n",
      "Epoch 736/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 400.1136 - val_loss: 1869.5421\n",
      "Epoch 737/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 395.3605 - val_loss: 1937.3133\n",
      "Epoch 738/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 366.9947 - val_loss: 2112.8885\n",
      "Epoch 739/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 389.3464 - val_loss: 1940.6562\n",
      "Epoch 740/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 328.3831 - val_loss: 2042.2840\n",
      "Epoch 741/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 359.3398 - val_loss: 2057.3896\n",
      "Epoch 742/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 421.6144 - val_loss: 2038.1798\n",
      "Epoch 743/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 397.8927 - val_loss: 1926.9459\n",
      "Epoch 744/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 371.2565 - val_loss: 1917.4924\n",
      "Epoch 745/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 397.7933 - val_loss: 2151.7640\n",
      "Epoch 746/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 361.0271 - val_loss: 1627.0292\n",
      "Epoch 747/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 369.6171 - val_loss: 2126.5547\n",
      "Epoch 748/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 335.0863 - val_loss: 2310.2585\n",
      "Epoch 749/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 375.6294 - val_loss: 1889.1388\n",
      "Epoch 750/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 322.3553 - val_loss: 2080.4916\n",
      "Epoch 751/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 356.4411 - val_loss: 1944.2264\n",
      "Epoch 752/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 336.6903 - val_loss: 1974.3694\n",
      "Epoch 753/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 362.2701 - val_loss: 1917.7650\n",
      "Epoch 754/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 323.4214 - val_loss: 1659.5067\n",
      "Epoch 755/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 339.5558 - val_loss: 1937.5752\n",
      "Epoch 756/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 331.5399 - val_loss: 2130.3765\n",
      "Epoch 757/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 357.7948 - val_loss: 2087.1488\n",
      "Epoch 758/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 297.3596 - val_loss: 1989.5784\n",
      "Epoch 759/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 330.2810 - val_loss: 1978.5630\n",
      "Epoch 760/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 318.5352 - val_loss: 1876.8762\n",
      "Epoch 761/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 298.2137 - val_loss: 1861.4556\n",
      "Epoch 762/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 289.9512 - val_loss: 1862.2054\n",
      "Epoch 763/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 299.0286 - val_loss: 1990.6158\n",
      "Epoch 764/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 290.6946 - val_loss: 1940.9063\n",
      "Epoch 765/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 299.4767 - val_loss: 1830.4029\n",
      "Epoch 766/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 305.3630 - val_loss: 2101.2019\n",
      "Epoch 767/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 322.4231 - val_loss: 2015.1002\n",
      "Epoch 768/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 301.4411 - val_loss: 2058.5389\n",
      "Epoch 769/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 294.9833 - val_loss: 1997.8167\n",
      "Epoch 770/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 331.3245 - val_loss: 1983.8153\n",
      "Epoch 771/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 320.5206 - val_loss: 1971.1195\n",
      "Epoch 772/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 274.1383 - val_loss: 2005.5245\n",
      "Epoch 773/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 306.4641 - val_loss: 1959.7455\n",
      "Epoch 774/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 328.0692 - val_loss: 1904.0217\n",
      "Epoch 775/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 293.0761 - val_loss: 2050.1514\n",
      "Epoch 776/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 297.0527 - val_loss: 1838.6613\n",
      "Epoch 777/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 299.5263 - val_loss: 2126.0428\n",
      "Epoch 778/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 278.7417 - val_loss: 2144.0620\n",
      "Epoch 779/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 329.7820 - val_loss: 2043.2129\n",
      "Epoch 780/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 328.5178 - val_loss: 1741.2031\n",
      "Epoch 781/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 311.5095 - val_loss: 1840.1332\n",
      "Epoch 782/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 303.0017 - val_loss: 2026.7812\n",
      "Epoch 783/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 314.7500 - val_loss: 1880.4323\n",
      "Epoch 784/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 285.5381 - val_loss: 1836.7411\n",
      "Epoch 785/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 258.5992 - val_loss: 2195.6401\n",
      "Epoch 786/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 330.9919 - val_loss: 1812.7574\n",
      "Epoch 787/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 354.8368 - val_loss: 1758.7367\n",
      "Epoch 788/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 290.0868 - val_loss: 2174.3484\n",
      "Epoch 789/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 308.7376 - val_loss: 1968.3135\n",
      "Epoch 790/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 356.4249 - val_loss: 1957.2290\n",
      "Epoch 791/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 332.8140 - val_loss: 1888.5583\n",
      "Epoch 792/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 326.4847 - val_loss: 1711.0510\n",
      "Epoch 793/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 317.1060 - val_loss: 2150.6792\n",
      "Epoch 794/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 310.9818 - val_loss: 2149.9176\n",
      "Epoch 795/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 378.5278 - val_loss: 2148.4638\n",
      "Epoch 796/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 326.6743 - val_loss: 1869.1462\n",
      "Epoch 797/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 301.4626 - val_loss: 2395.4431\n",
      "Epoch 798/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 366.4662 - val_loss: 1730.6027\n",
      "Epoch 799/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 330.8017 - val_loss: 2194.9629\n",
      "Epoch 800/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 294.6398 - val_loss: 1891.6800\n",
      "Epoch 801/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 287.6347 - val_loss: 2053.9535\n",
      "Epoch 802/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 285.3744 - val_loss: 1709.4640\n",
      "Epoch 803/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 278.1992 - val_loss: 2150.2095\n",
      "Epoch 804/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 266.1723 - val_loss: 2135.7549\n",
      "Epoch 805/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 251.0251 - val_loss: 1671.8082\n",
      "Epoch 806/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 236.3926 - val_loss: 1988.2390\n",
      "Epoch 807/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 264.2915 - val_loss: 1977.4980\n",
      "Epoch 808/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 247.4890 - val_loss: 1923.0134\n",
      "Epoch 809/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 250.8186 - val_loss: 2019.1939\n",
      "Epoch 810/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 244.4240 - val_loss: 2027.5958\n",
      "Epoch 811/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 265.3745 - val_loss: 1863.7749\n",
      "Epoch 812/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 333.3196 - val_loss: 2009.3172\n",
      "Epoch 813/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 350.4072 - val_loss: 2097.9553\n",
      "Epoch 814/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 341.0576 - val_loss: 1627.6233\n",
      "Epoch 815/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 373.2445 - val_loss: 2270.0367\n",
      "Epoch 816/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 337.4492 - val_loss: 1814.1811\n",
      "Epoch 817/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 410.4247 - val_loss: 1826.3406\n",
      "Epoch 818/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 520.4107 - val_loss: 2251.4520\n",
      "Epoch 819/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 459.0651 - val_loss: 1545.0647\n",
      "Epoch 820/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 374.0140 - val_loss: 2485.2440\n",
      "Epoch 821/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 586.9588 - val_loss: 1815.4071\n",
      "Epoch 822/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 486.2658 - val_loss: 2166.2143\n",
      "Epoch 823/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 780.5878 - val_loss: 2229.7508\n",
      "Epoch 824/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 496.9115 - val_loss: 1961.2974\n",
      "Epoch 825/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 499.3146 - val_loss: 1876.7226\n",
      "Epoch 826/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 345.2997 - val_loss: 1825.4215\n",
      "Epoch 827/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 440.2417 - val_loss: 1536.6071\n",
      "Epoch 828/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 443.2628 - val_loss: 1881.1676\n",
      "Epoch 829/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 476.1089 - val_loss: 1977.5020\n",
      "Epoch 830/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 486.5248 - val_loss: 1862.9176\n",
      "Epoch 831/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 358.9542 - val_loss: 1903.4238\n",
      "Epoch 832/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 383.3145 - val_loss: 1712.3115\n",
      "Epoch 833/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 372.5912 - val_loss: 1882.6055\n",
      "Epoch 834/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 339.9802 - val_loss: 1756.0628\n",
      "Epoch 835/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 445.9660 - val_loss: 2181.3150\n",
      "Epoch 836/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 419.2872 - val_loss: 2119.9959\n",
      "Epoch 837/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 380.2652 - val_loss: 1698.0909\n",
      "Epoch 838/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 395.9401 - val_loss: 1973.5102\n",
      "Epoch 839/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 373.2821 - val_loss: 1684.2439\n",
      "Epoch 840/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 397.2860 - val_loss: 2113.0420\n",
      "Epoch 841/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 333.9879 - val_loss: 1774.5492\n",
      "Epoch 842/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 273.9193 - val_loss: 1752.1387\n",
      "Epoch 843/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 270.0546 - val_loss: 1825.6887\n",
      "Epoch 844/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 263.6047 - val_loss: 1838.6358\n",
      "Epoch 845/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 267.1357 - val_loss: 1812.9900\n",
      "Epoch 846/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 252.7165 - val_loss: 1912.6244\n",
      "Epoch 847/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 246.6359 - val_loss: 1777.7642\n",
      "Epoch 848/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 252.2552 - val_loss: 1955.8817\n",
      "Epoch 849/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 245.2631 - val_loss: 2010.4652\n",
      "Epoch 850/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 260.2355 - val_loss: 1881.3406\n",
      "Epoch 851/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 229.5086 - val_loss: 1885.9379\n",
      "Epoch 852/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 227.7017 - val_loss: 1882.4838\n",
      "Epoch 853/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 222.8905 - val_loss: 1962.2279\n",
      "Epoch 854/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 234.8446 - val_loss: 1868.0947\n",
      "Epoch 855/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 261.7107 - val_loss: 2184.1956\n",
      "Epoch 856/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 222.2052 - val_loss: 1542.8224\n",
      "Epoch 857/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 245.1764 - val_loss: 2158.2658\n",
      "Epoch 858/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 252.0650 - val_loss: 1840.6711\n",
      "Epoch 859/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 246.4750 - val_loss: 2092.5108\n",
      "Epoch 860/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 233.3792 - val_loss: 2032.6630\n",
      "Epoch 861/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 222.0290 - val_loss: 1785.7358\n",
      "Epoch 862/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 208.1511 - val_loss: 2070.5685\n",
      "Epoch 863/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 216.3111 - val_loss: 2015.0115\n",
      "Epoch 864/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 214.4569 - val_loss: 2016.7495\n",
      "Epoch 865/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 219.5245 - val_loss: 2070.6811\n",
      "Epoch 866/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 202.8675 - val_loss: 2109.8723\n",
      "Epoch 867/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 221.5827 - val_loss: 2055.6300\n",
      "Epoch 868/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 196.0113 - val_loss: 1892.5239\n",
      "Epoch 869/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 224.7984 - val_loss: 2011.8642\n",
      "Epoch 870/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 229.0577 - val_loss: 1881.6675\n",
      "Epoch 871/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 224.6405 - val_loss: 2187.8872\n",
      "Epoch 872/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 225.1257 - val_loss: 1991.5958\n",
      "Epoch 873/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 216.5018 - val_loss: 2224.5564\n",
      "Epoch 874/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 229.2444 - val_loss: 1918.7504\n",
      "Epoch 875/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 223.9781 - val_loss: 2227.7873\n",
      "Epoch 876/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 203.9229 - val_loss: 2088.3423\n",
      "Epoch 877/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 236.8539 - val_loss: 2166.2346\n",
      "Epoch 878/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 241.5577 - val_loss: 2201.9491\n",
      "Epoch 879/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 245.9014 - val_loss: 1898.6219\n",
      "Epoch 880/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 287.6631 - val_loss: 2075.6880\n",
      "Epoch 881/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 329.5693 - val_loss: 1565.9314\n",
      "Epoch 882/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 363.0076 - val_loss: 2443.8392\n",
      "Epoch 883/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 493.3603 - val_loss: 2177.6534\n",
      "Epoch 884/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 469.0467 - val_loss: 1753.5453\n",
      "Epoch 885/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 349.4749 - val_loss: 2061.3825\n",
      "Epoch 886/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 313.1887 - val_loss: 1595.0474\n",
      "Epoch 887/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 585.1336 - val_loss: 1914.2476\n",
      "Epoch 888/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 490.1750 - val_loss: 2402.9988\n",
      "Epoch 889/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 549.6548 - val_loss: 2132.6411\n",
      "Epoch 890/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 558.0577 - val_loss: 1664.1189\n",
      "Epoch 891/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 414.6073 - val_loss: 2121.2664\n",
      "Epoch 892/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 426.1178 - val_loss: 2003.6286\n",
      "Epoch 893/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 384.0648 - val_loss: 1905.8678\n",
      "Epoch 894/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 605.4238 - val_loss: 2060.4332\n",
      "Epoch 895/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 552.9528 - val_loss: 2516.7549\n",
      "Epoch 896/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 2ms/step - loss: 620.1517 - val_loss: 2290.8304\n",
      "Epoch 897/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 542.2920 - val_loss: 2083.9192\n",
      "Epoch 898/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 487.0465 - val_loss: 2529.9140\n",
      "Epoch 899/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 523.4458 - val_loss: 2560.6899\n",
      "Epoch 900/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 645.3220 - val_loss: 2051.7748\n",
      "Epoch 901/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 489.1429 - val_loss: 1925.1762\n",
      "Epoch 902/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 625.3556 - val_loss: 1846.4769\n",
      "Epoch 903/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 620.6288 - val_loss: 1838.4026\n",
      "Epoch 904/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 529.8818 - val_loss: 2132.4327\n",
      "Epoch 905/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 638.3433 - val_loss: 2624.1331\n",
      "Epoch 906/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 705.1428 - val_loss: 2241.4812\n",
      "Epoch 907/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 574.7382 - val_loss: 1814.7804\n",
      "Epoch 908/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 688.0415 - val_loss: 1651.5400\n",
      "Epoch 909/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 400.0115 - val_loss: 2435.9130\n",
      "Epoch 910/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 512.9470 - val_loss: 2138.0459\n",
      "Epoch 911/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 373.7343 - val_loss: 1797.6342\n",
      "Epoch 912/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 426.4841 - val_loss: 2066.7880\n",
      "Epoch 913/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 315.0030 - val_loss: 2385.7821\n",
      "Epoch 914/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 296.6632 - val_loss: 1978.7234\n",
      "Epoch 915/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 277.6797 - val_loss: 1808.8557\n",
      "Epoch 916/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 255.9746 - val_loss: 2097.2871\n",
      "Epoch 917/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 246.7734 - val_loss: 2001.3924\n",
      "Epoch 918/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 244.9624 - val_loss: 2233.8764\n",
      "Epoch 919/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 235.8009 - val_loss: 2161.3720\n",
      "Epoch 920/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 237.9191 - val_loss: 1786.4020\n",
      "Epoch 921/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 229.6425 - val_loss: 2159.1553\n",
      "Epoch 922/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 237.9060 - val_loss: 1928.4735\n",
      "Epoch 923/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 235.3399 - val_loss: 1816.3289\n",
      "Epoch 924/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 249.4239 - val_loss: 2115.8080\n",
      "Epoch 925/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 248.2982 - val_loss: 1933.4866\n",
      "Epoch 926/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 232.1436 - val_loss: 2269.7804\n",
      "Epoch 927/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 254.0382 - val_loss: 1953.6782\n",
      "Epoch 928/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 225.4093 - val_loss: 1944.9845\n",
      "Epoch 929/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 226.1445 - val_loss: 2159.2605\n",
      "Epoch 930/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 210.4564 - val_loss: 1946.3296\n",
      "Epoch 931/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 219.6695 - val_loss: 2019.8269\n",
      "Epoch 932/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 200.0728 - val_loss: 1977.6667\n",
      "Epoch 933/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 204.7845 - val_loss: 1982.4727\n",
      "Epoch 934/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 224.0059 - val_loss: 2111.7581\n",
      "Epoch 935/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 216.4803 - val_loss: 1898.6323\n",
      "Epoch 936/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 190.9637 - val_loss: 2086.3396\n",
      "Epoch 937/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 205.7900 - val_loss: 2023.1495\n",
      "Epoch 938/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 192.5334 - val_loss: 2035.9675\n",
      "Epoch 939/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 185.0777 - val_loss: 2079.2337\n",
      "Epoch 940/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 188.8219 - val_loss: 2015.1972\n",
      "Epoch 941/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 191.0418 - val_loss: 2040.4208\n",
      "Epoch 942/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 184.9670 - val_loss: 2130.5091\n",
      "Epoch 943/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 175.0698 - val_loss: 2029.3179\n",
      "Epoch 944/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 198.0135 - val_loss: 2096.7045\n",
      "Epoch 945/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 194.8342 - val_loss: 2076.3993\n",
      "Epoch 946/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 178.3072 - val_loss: 2077.1566\n",
      "Epoch 947/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 174.1548 - val_loss: 2075.1986\n",
      "Epoch 948/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 175.2352 - val_loss: 2123.2546\n",
      "Epoch 949/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 178.0045 - val_loss: 2154.2978\n",
      "Epoch 950/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 180.1597 - val_loss: 2040.9522\n",
      "Epoch 951/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 174.8607 - val_loss: 2122.6026\n",
      "Epoch 952/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 191.8949 - val_loss: 2140.3283\n",
      "Epoch 953/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 181.2689 - val_loss: 2084.7503\n",
      "Epoch 954/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 197.6157 - val_loss: 1866.0480\n",
      "Epoch 955/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 229.9938 - val_loss: 2328.6389\n",
      "Epoch 956/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 233.7605 - val_loss: 2061.8342\n",
      "Epoch 957/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 204.5006 - val_loss: 2408.8171\n",
      "Epoch 958/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 220.0661 - val_loss: 2174.2922\n",
      "Epoch 959/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 189.2580 - val_loss: 2088.8276\n",
      "Epoch 960/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 215.6597 - val_loss: 2169.7400\n",
      "Epoch 961/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 257.5658 - val_loss: 1802.9428\n",
      "Epoch 962/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 307.8214 - val_loss: 2309.7502\n",
      "Epoch 963/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 391.9918 - val_loss: 2447.8265\n",
      "Epoch 964/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 411.0870 - val_loss: 2031.1548\n",
      "Epoch 965/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 292.0278 - val_loss: 2129.8665\n",
      "Epoch 966/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 280.8333 - val_loss: 1881.4260\n",
      "Epoch 967/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 221.5374 - val_loss: 2107.0293\n",
      "Epoch 968/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 222.6887 - val_loss: 2058.0336\n",
      "Epoch 969/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 247.2959 - val_loss: 2013.5228\n",
      "Epoch 970/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 222.1375 - val_loss: 2172.6693\n",
      "Epoch 971/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 200.5777 - val_loss: 2251.3506\n",
      "Epoch 972/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 199.9601 - val_loss: 2089.6211\n",
      "Epoch 973/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 197.7483 - val_loss: 2588.4987\n",
      "Epoch 974/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 212.2241 - val_loss: 2038.9359\n",
      "Epoch 975/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 195.1807 - val_loss: 2160.0374\n",
      "Epoch 976/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 218.5607 - val_loss: 2139.3185\n",
      "Epoch 977/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 198.8248 - val_loss: 2213.4617\n",
      "Epoch 978/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 200.7082 - val_loss: 2255.5266\n",
      "Epoch 979/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 194.1942 - val_loss: 2065.4355\n",
      "Epoch 980/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 206.0234 - val_loss: 2339.6442\n",
      "Epoch 981/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 203.6754 - val_loss: 2137.8592\n",
      "Epoch 982/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 197.1932 - val_loss: 2137.3794\n",
      "Epoch 983/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 185.6265 - val_loss: 2245.3743\n",
      "Epoch 984/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 171.7490 - val_loss: 2054.7056\n",
      "Epoch 985/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 177.3596 - val_loss: 2243.3512\n",
      "Epoch 986/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 179.6429 - val_loss: 2128.0286\n",
      "Epoch 987/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 179.3412 - val_loss: 2196.7391\n",
      "Epoch 988/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 155.5203 - val_loss: 2295.2421\n",
      "Epoch 989/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 168.3518 - val_loss: 2271.8142\n",
      "Epoch 990/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 171.3318 - val_loss: 2168.9150\n",
      "Epoch 991/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 169.0551 - val_loss: 2370.1476\n",
      "Epoch 992/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 180.3443 - val_loss: 2063.4919\n",
      "Epoch 993/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 160.5062 - val_loss: 2402.8659\n",
      "Epoch 994/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 154.5130 - val_loss: 2106.4141\n",
      "Epoch 995/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 176.3438 - val_loss: 2318.3527\n",
      "Epoch 996/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 178.6950 - val_loss: 1981.9956\n",
      "Epoch 997/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 162.0311 - val_loss: 2406.1884\n",
      "Epoch 998/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 171.0864 - val_loss: 1989.2041\n",
      "Epoch 999/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 177.8637 - val_loss: 2283.3218\n",
      "Epoch 1000/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 162.9203 - val_loss: 2148.8076\n",
      "Epoch 1001/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 182.9351 - val_loss: 2175.7859\n",
      "Epoch 1002/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 174.9879 - val_loss: 2170.2275\n",
      "Epoch 1003/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 175.6170 - val_loss: 2141.0284\n",
      "Epoch 1004/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 178.9323 - val_loss: 2461.5333\n",
      "Epoch 1005/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 166.9337 - val_loss: 2091.1209\n",
      "Epoch 1006/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 185.5653 - val_loss: 2637.0081\n",
      "Epoch 1007/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 212.9806 - val_loss: 2077.4498\n",
      "Epoch 1008/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 245.4339 - val_loss: 2650.6483\n",
      "Epoch 1009/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 301.4101 - val_loss: 1899.9593\n",
      "Epoch 1010/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 383.4830 - val_loss: 2657.8259\n",
      "Epoch 1011/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 371.1898 - val_loss: 2343.3562\n",
      "Epoch 1012/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 399.4278 - val_loss: 2196.6426\n",
      "Epoch 1013/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 327.9982 - val_loss: 2223.6880\n",
      "Epoch 1014/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 260.9914 - val_loss: 2152.2603\n",
      "Epoch 1015/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 264.1813 - val_loss: 2499.6689\n",
      "Epoch 1016/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 281.9924 - val_loss: 2341.9466\n",
      "Epoch 1017/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 375.7016 - val_loss: 2155.8779\n",
      "Epoch 1018/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 242.5202 - val_loss: 2691.9332\n",
      "Epoch 1019/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 268.0531 - val_loss: 2148.0744\n",
      "Epoch 1020/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 253.9822 - val_loss: 2316.5456\n",
      "Epoch 1021/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 240.0382 - val_loss: 2395.6573\n",
      "Epoch 1022/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 231.7921 - val_loss: 2448.4864\n",
      "Epoch 1023/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 220.8645 - val_loss: 2117.1344\n",
      "Epoch 1024/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 184.5589 - val_loss: 2486.3760\n",
      "Epoch 1025/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 211.3334 - val_loss: 2162.8150\n",
      "Epoch 1026/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 196.2372 - val_loss: 2342.5496\n",
      "Epoch 1027/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 181.3024 - val_loss: 2440.9453\n",
      "Epoch 1028/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 189.6230 - val_loss: 2305.5434\n",
      "Epoch 1029/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 193.1365 - val_loss: 2233.9465\n",
      "Epoch 1030/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 168.1106 - val_loss: 2419.8473\n",
      "Epoch 1031/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 172.1473 - val_loss: 2383.8242\n",
      "Epoch 1032/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 165.0043 - val_loss: 2112.9523\n",
      "Epoch 1033/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 184.5754 - val_loss: 2406.0967\n",
      "Epoch 1034/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 181.3117 - val_loss: 2147.6114\n",
      "Epoch 1035/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 198.8471 - val_loss: 2524.4952\n",
      "Epoch 1036/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 170.9621 - val_loss: 2147.0692\n",
      "Epoch 1037/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 173.0044 - val_loss: 2565.2917\n",
      "Epoch 1038/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 175.1574 - val_loss: 2025.8401\n",
      "Epoch 1039/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 197.3673 - val_loss: 2546.3159\n",
      "Epoch 1040/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 200.1936 - val_loss: 2194.4777\n",
      "Epoch 1041/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 205.1042 - val_loss: 2238.1149\n",
      "Epoch 1042/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 155.6183 - val_loss: 2534.8993\n",
      "Epoch 1043/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 183.8156 - val_loss: 2111.7922\n",
      "Epoch 1044/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 160.7446 - val_loss: 2478.6878\n",
      "Epoch 1045/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 177.1753 - val_loss: 2520.9714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1046/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 206.3713 - val_loss: 2525.3234\n",
      "Epoch 1047/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 327.5917 - val_loss: 1874.9129\n",
      "Epoch 1048/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 356.6221 - val_loss: 2342.0025\n",
      "Epoch 1049/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 388.3314 - val_loss: 1992.9928\n",
      "Epoch 1050/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 322.1775 - val_loss: 1789.7243\n",
      "Epoch 1051/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 276.9706 - val_loss: 2313.4841\n",
      "Epoch 1052/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 251.2401 - val_loss: 2385.0959\n",
      "Epoch 1053/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 258.0333 - val_loss: 2277.7970\n",
      "Epoch 1054/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 242.0743 - val_loss: 2157.8054\n",
      "Epoch 1055/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 216.3329 - val_loss: 2268.9822\n",
      "Epoch 1056/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 176.0609 - val_loss: 2431.3037\n",
      "Epoch 1057/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 180.7301 - val_loss: 2084.2831\n",
      "Epoch 1058/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 176.5098 - val_loss: 2150.4761\n",
      "Epoch 1059/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 163.5868 - val_loss: 2317.0764\n",
      "Epoch 1060/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 177.7799 - val_loss: 2219.1859\n",
      "Epoch 1061/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 228.7610 - val_loss: 2259.5472\n",
      "Epoch 1062/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 218.0281 - val_loss: 2892.5992\n",
      "Epoch 1063/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 211.4167 - val_loss: 1846.8215\n",
      "Epoch 1064/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 233.4765 - val_loss: 2271.7010\n",
      "Epoch 1065/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 186.9818 - val_loss: 2378.0807\n",
      "Epoch 1066/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 172.2278 - val_loss: 2207.1906\n",
      "Epoch 1067/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 178.2692 - val_loss: 2436.5142\n",
      "Epoch 1068/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 170.8079 - val_loss: 2145.9780\n",
      "Epoch 1069/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 170.8424 - val_loss: 2278.5905\n",
      "Epoch 1070/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 145.9533 - val_loss: 2225.6243\n",
      "Epoch 1071/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 157.1567 - val_loss: 2364.0053\n",
      "Epoch 1072/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 159.6114 - val_loss: 2356.7365\n",
      "Epoch 1073/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 166.7486 - val_loss: 2297.2821\n",
      "Epoch 1074/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 125.7772 - val_loss: 2430.6809\n",
      "Epoch 1075/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 138.4941 - val_loss: 2277.9384\n",
      "Epoch 1076/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 137.1257 - val_loss: 2337.7290\n",
      "Epoch 1077/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 134.5287 - val_loss: 2349.4927\n",
      "Epoch 1078/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 149.7746 - val_loss: 2258.3999\n",
      "Epoch 1079/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 156.6684 - val_loss: 2335.2121\n",
      "Epoch 1080/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 128.9313 - val_loss: 2300.6665\n",
      "Epoch 1081/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 135.4264 - val_loss: 2357.0659\n",
      "Epoch 1082/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 156.2811 - val_loss: 2279.3726\n",
      "Epoch 1083/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 157.0020 - val_loss: 2330.9456\n",
      "Epoch 1084/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 138.9432 - val_loss: 2390.2639\n",
      "Epoch 1085/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 159.2350 - val_loss: 2109.8416\n",
      "Epoch 1086/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 150.8328 - val_loss: 2341.4367\n",
      "Epoch 1087/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 189.4908 - val_loss: 2053.8006\n",
      "Epoch 1088/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 261.5660 - val_loss: 2397.1621\n",
      "Epoch 1089/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 219.6163 - val_loss: 2192.1671\n",
      "Epoch 1090/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 277.4347 - val_loss: 2012.1683\n",
      "Epoch 1091/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 215.3882 - val_loss: 2494.6640\n",
      "Epoch 1092/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 210.7964 - val_loss: 2067.8039\n",
      "Epoch 1093/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 181.7137 - val_loss: 2281.7837\n",
      "Epoch 1094/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 186.7339 - val_loss: 2104.7815\n",
      "Epoch 1095/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 165.6174 - val_loss: 2394.7773\n",
      "Epoch 1096/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 164.3326 - val_loss: 2208.6740\n",
      "Epoch 1097/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 169.9610 - val_loss: 2227.7925\n",
      "Epoch 1098/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 187.7149 - val_loss: 2203.1868\n",
      "Epoch 1099/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 135.8168 - val_loss: 2196.2957\n",
      "Epoch 1100/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 147.2083 - val_loss: 2337.5495\n",
      "Epoch 1101/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 163.6001 - val_loss: 2201.1506\n",
      "Epoch 1102/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 141.1783 - val_loss: 2283.3379\n",
      "Epoch 1103/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 140.9013 - val_loss: 2235.8619\n",
      "Epoch 1104/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 136.7583 - val_loss: 2223.9382\n",
      "Epoch 1105/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 120.2910 - val_loss: 2238.9656\n",
      "Epoch 1106/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 138.5809 - val_loss: 2334.4873\n",
      "Epoch 1107/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 126.5075 - val_loss: 2206.3513\n",
      "Epoch 1108/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 140.3523 - val_loss: 2674.9676\n",
      "Epoch 1109/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 181.6590 - val_loss: 2262.4223\n",
      "Epoch 1110/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 131.6581 - val_loss: 2398.4257\n",
      "Epoch 1111/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 138.5425 - val_loss: 2059.8759\n",
      "Epoch 1112/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 161.1613 - val_loss: 2310.8571\n",
      "Epoch 1113/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 143.5151 - val_loss: 2291.2839\n",
      "Epoch 1114/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 151.8574 - val_loss: 2330.4153\n",
      "Epoch 1115/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 161.1683 - val_loss: 2361.2882\n",
      "Epoch 1116/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 151.5298 - val_loss: 2380.6098\n",
      "Epoch 1117/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 151.0517 - val_loss: 2167.9426\n",
      "Epoch 1118/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 148.9019 - val_loss: 2293.2119\n",
      "Epoch 1119/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 162.6080 - val_loss: 2421.4819\n",
      "Epoch 1120/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 149.2056 - val_loss: 2198.1579\n",
      "Epoch 1121/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 205.7536 - val_loss: 2461.0416\n",
      "Epoch 1122/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 176.2726 - val_loss: 2100.9088\n",
      "Epoch 1123/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 150.6746 - val_loss: 2471.5792\n",
      "Epoch 1124/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 147.1677 - val_loss: 2182.1176\n",
      "Epoch 1125/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 141.4139 - val_loss: 2386.6404\n",
      "Epoch 1126/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 160.2109 - val_loss: 2465.5867\n",
      "Epoch 1127/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 152.8175 - val_loss: 2157.4302\n",
      "Epoch 1128/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 161.8292 - val_loss: 2452.3636\n",
      "Epoch 1129/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 148.5709 - val_loss: 2060.5983\n",
      "Epoch 1130/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 144.9327 - val_loss: 2434.9080\n",
      "Epoch 1131/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 145.0379 - val_loss: 2183.9382\n",
      "Epoch 1132/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 152.0452 - val_loss: 2393.2640\n",
      "Epoch 1133/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 149.1731 - val_loss: 2355.6654\n",
      "Epoch 1134/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 130.3392 - val_loss: 2403.8907\n",
      "Epoch 1135/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 132.2386 - val_loss: 2317.5237\n",
      "Epoch 1136/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 133.7558 - val_loss: 2395.6054\n",
      "Epoch 1137/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 135.8859 - val_loss: 2242.2507\n",
      "Epoch 1138/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 148.7128 - val_loss: 2285.0122\n",
      "Epoch 1139/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 124.5695 - val_loss: 2280.8437\n",
      "Epoch 1140/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 124.7819 - val_loss: 2211.1028\n",
      "Epoch 1141/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 123.7024 - val_loss: 2427.6468\n",
      "Epoch 1142/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 135.1439 - val_loss: 2242.4300\n",
      "Epoch 1143/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 136.7601 - val_loss: 2487.2510\n",
      "Epoch 1144/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 121.8092 - val_loss: 2173.4865\n",
      "Epoch 1145/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 114.3941 - val_loss: 2764.1246\n",
      "Epoch 1146/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 142.6071 - val_loss: 2189.9196\n",
      "Epoch 1147/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 158.7603 - val_loss: 2437.3886\n",
      "Epoch 1148/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 148.1898 - val_loss: 2122.8135\n",
      "Epoch 1149/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 157.3838 - val_loss: 2248.7588\n",
      "Epoch 1150/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 150.9936 - val_loss: 2410.0477\n",
      "Epoch 1151/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 177.7054 - val_loss: 2383.0422\n",
      "Epoch 1152/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 185.8221 - val_loss: 2575.7904\n",
      "Epoch 1153/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 197.6876 - val_loss: 2354.0200\n",
      "Epoch 1154/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 245.7088 - val_loss: 2038.6883\n",
      "Epoch 1155/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 246.5571 - val_loss: 2564.0619\n",
      "Epoch 1156/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 281.0824 - val_loss: 2137.7355\n",
      "Epoch 1157/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 266.9374 - val_loss: 2453.7786\n",
      "Epoch 1158/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 263.4614 - val_loss: 2126.9281\n",
      "Epoch 1159/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 189.9938 - val_loss: 2413.6159\n",
      "Epoch 1160/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 227.4495 - val_loss: 2086.3887\n",
      "Epoch 1161/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 184.9492 - val_loss: 2400.3738\n",
      "Epoch 1162/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 171.6367 - val_loss: 2083.1585\n",
      "Epoch 1163/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 179.5524 - val_loss: 2190.7998\n",
      "Epoch 1164/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 159.0868 - val_loss: 2102.2443\n",
      "Epoch 1165/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 133.7088 - val_loss: 2391.4813\n",
      "Epoch 1166/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 164.5445 - val_loss: 2113.6467\n",
      "Epoch 1167/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 142.9197 - val_loss: 2175.6679\n",
      "Epoch 1168/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 134.1599 - val_loss: 2257.9076\n",
      "Epoch 1169/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 137.7370 - val_loss: 2280.7271\n",
      "Epoch 1170/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 137.1089 - val_loss: 1956.5536\n",
      "Epoch 1171/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 550.4046 - val_loss: 2241.0827\n",
      "Epoch 1172/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 586.6608 - val_loss: 2263.3787\n",
      "Epoch 1173/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 474.8328 - val_loss: 2211.7380\n",
      "Epoch 1174/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 421.8337 - val_loss: 2357.1886\n",
      "Epoch 1175/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 381.9124 - val_loss: 2175.2872\n",
      "Epoch 1176/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 388.3308 - val_loss: 2085.4958\n",
      "Epoch 1177/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 333.7393 - val_loss: 2287.1653\n",
      "Epoch 1178/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 307.9032 - val_loss: 2083.6122\n",
      "Epoch 1179/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 233.3886 - val_loss: 2340.7252\n",
      "Epoch 1180/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 275.3047 - val_loss: 2080.7627\n",
      "Epoch 1181/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 238.4357 - val_loss: 2172.7746\n",
      "Epoch 1182/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 180.6711 - val_loss: 2189.8149\n",
      "Epoch 1183/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 192.4529 - val_loss: 2138.6893\n",
      "Epoch 1184/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 185.2908 - val_loss: 2357.7027\n",
      "Epoch 1185/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 162.2439 - val_loss: 2221.7031\n",
      "Epoch 1186/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 166.8167 - val_loss: 2266.2892\n",
      "Epoch 1187/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 154.1998 - val_loss: 2296.9209\n",
      "Epoch 1188/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 125.4196 - val_loss: 2267.3636\n",
      "Epoch 1189/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 137.5740 - val_loss: 2393.9876\n",
      "Epoch 1190/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 130.3874 - val_loss: 2302.2641\n",
      "Epoch 1191/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 143.4152 - val_loss: 2356.1386\n",
      "Epoch 1192/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 143.0947 - val_loss: 2100.0798\n",
      "Epoch 1193/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 166.6787 - val_loss: 2292.8519\n",
      "Epoch 1194/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 140.1077 - val_loss: 2184.4433\n",
      "Epoch 1195/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 140.5081 - val_loss: 2103.8787\n",
      "Epoch 1196/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 157.5849 - val_loss: 2340.6374\n",
      "Epoch 1197/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 230.0181 - val_loss: 2322.2668\n",
      "Epoch 1198/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 224.3245 - val_loss: 1920.2419\n",
      "Epoch 1199/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 298.5205 - val_loss: 2506.2066\n",
      "Epoch 1200/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 236.0142 - val_loss: 1969.3480\n",
      "Epoch 1201/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 228.5990 - val_loss: 2343.7600\n",
      "Epoch 1202/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 207.0731 - val_loss: 2364.1306\n",
      "Epoch 1203/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 184.4908 - val_loss: 2284.9955\n",
      "Epoch 1204/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 160.1894 - val_loss: 2324.1137\n",
      "Epoch 1205/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 175.2237 - val_loss: 2249.3829\n",
      "Epoch 1206/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 159.6718 - val_loss: 2251.7748\n",
      "Epoch 1207/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 162.1370 - val_loss: 2765.8549\n",
      "Epoch 1208/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 232.5417 - val_loss: 1970.7541\n",
      "Epoch 1209/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 212.9947 - val_loss: 2503.0309\n",
      "Epoch 1210/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 176.8645 - val_loss: 2691.5932\n",
      "Epoch 1211/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 162.9744 - val_loss: 2426.0513\n",
      "Epoch 1212/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 158.2875 - val_loss: 2475.0173\n",
      "Epoch 1213/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 170.1792 - val_loss: 2354.6726\n",
      "Epoch 1214/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 162.9514 - val_loss: 2669.7755\n",
      "Epoch 1215/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 147.5862 - val_loss: 2329.2371\n",
      "Epoch 1216/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 136.7523 - val_loss: 2274.2412\n",
      "Epoch 1217/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 129.8551 - val_loss: 2441.3484\n",
      "Epoch 1218/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 123.1239 - val_loss: 2244.3061\n",
      "Epoch 1219/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 146.0008 - val_loss: 2311.6474\n",
      "Epoch 1220/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 126.1208 - val_loss: 2362.3382\n",
      "Epoch 1221/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 119.2709 - val_loss: 2320.1546\n",
      "Epoch 1222/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 107.7984 - val_loss: 2560.3809\n",
      "Epoch 1223/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 133.2720 - val_loss: 2348.9208\n",
      "Epoch 1224/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 104.6086 - val_loss: 2390.4554\n",
      "Epoch 1225/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 108.6354 - val_loss: 2359.9538\n",
      "Epoch 1226/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 97.9036 - val_loss: 2447.5395\n",
      "Epoch 1227/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 112.0595 - val_loss: 2397.3592\n",
      "Epoch 1228/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 121.7637 - val_loss: 2382.1466\n",
      "Epoch 1229/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 131.5451 - val_loss: 2318.0387\n",
      "Epoch 1230/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 132.0865 - val_loss: 2374.4555\n",
      "Epoch 1231/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 113.6361 - val_loss: 2280.2809\n",
      "Epoch 1232/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 108.4120 - val_loss: 2448.2545\n",
      "Epoch 1233/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 123.6737 - val_loss: 2281.6823\n",
      "Epoch 1234/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 128.2313 - val_loss: 2474.9689\n",
      "Epoch 1235/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 144.7765 - val_loss: 2250.0280\n",
      "Epoch 1236/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 166.3005 - val_loss: 2501.5363\n",
      "Epoch 1237/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 169.3127 - val_loss: 2304.1889\n",
      "Epoch 1238/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 193.6012 - val_loss: 2348.5016\n",
      "Epoch 1239/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 183.2636 - val_loss: 2311.2375\n",
      "Epoch 1240/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 215.4998 - val_loss: 2366.1171\n",
      "Epoch 1241/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 216.2623 - val_loss: 2424.4723\n",
      "Epoch 1242/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 179.5821 - val_loss: 2417.6677\n",
      "Epoch 1243/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 175.4775 - val_loss: 2149.5460\n",
      "Epoch 1244/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 265.0448 - val_loss: 2618.5676\n",
      "Epoch 1245/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 282.8210 - val_loss: 2566.5947\n",
      "Epoch 1246/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 226.8406 - val_loss: 1664.7296\n",
      "Epoch 1247/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 252.0500 - val_loss: 2527.7843\n",
      "Epoch 1248/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 214.7611 - val_loss: 2196.5674\n",
      "Epoch 1249/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 190.3079 - val_loss: 2280.2997\n",
      "Epoch 1250/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 230.9737 - val_loss: 2438.3436\n",
      "Epoch 1251/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 169.2903 - val_loss: 2093.6393\n",
      "Epoch 1252/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 155.6904 - val_loss: 2708.7292\n",
      "Epoch 1253/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 164.9868 - val_loss: 2376.9377\n",
      "Epoch 1254/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 160.8641 - val_loss: 2253.7521\n",
      "Epoch 1255/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 175.8205 - val_loss: 2569.6901\n",
      "Epoch 1256/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 160.2635 - val_loss: 2208.1473\n",
      "Epoch 1257/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 139.2287 - val_loss: 2454.6752\n",
      "Epoch 1258/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 158.1979 - val_loss: 2585.6599\n",
      "Epoch 1259/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 164.2711 - val_loss: 2344.1505\n",
      "Epoch 1260/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 170.4708 - val_loss: 2557.7998\n",
      "Epoch 1261/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 137.0592 - val_loss: 2502.4777\n",
      "Epoch 1262/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 135.6809 - val_loss: 2073.3447\n",
      "Epoch 1263/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 159.6639 - val_loss: 2843.8196\n",
      "Epoch 1264/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 173.9392 - val_loss: 2436.1892\n",
      "Epoch 1265/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 194.7602 - val_loss: 2859.8365\n",
      "Epoch 1266/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 169.0189 - val_loss: 2708.8588\n",
      "Epoch 1267/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 193.4543 - val_loss: 2138.9261\n",
      "Epoch 1268/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 162.7791 - val_loss: 2550.5851\n",
      "Epoch 1269/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 144.3567 - val_loss: 2269.4579\n",
      "Epoch 1270/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 136.3681 - val_loss: 2333.5357\n",
      "Epoch 1271/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 132.7596 - val_loss: 2692.2831\n",
      "Epoch 1272/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 122.7879 - val_loss: 2304.0230\n",
      "Epoch 1273/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 133.4857 - val_loss: 2515.0595\n",
      "Epoch 1274/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 118.5116 - val_loss: 2498.0669\n",
      "Epoch 1275/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 95.2437 - val_loss: 2396.7868\n",
      "Epoch 1276/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 115.6882 - val_loss: 2564.0898\n",
      "Epoch 1277/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 104.0618 - val_loss: 2456.7707\n",
      "Epoch 1278/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 113.1446 - val_loss: 2418.8158\n",
      "Epoch 1279/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 94.1252 - val_loss: 2580.0334\n",
      "Epoch 1280/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 102.4117 - val_loss: 2353.9949\n",
      "Epoch 1281/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 109.7453 - val_loss: 2650.2322\n",
      "Epoch 1282/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 97.9984 - val_loss: 2398.8546\n",
      "Epoch 1283/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 101.1636 - val_loss: 2531.6677\n",
      "Epoch 1284/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 98.8272 - val_loss: 2509.7059\n",
      "Epoch 1285/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 82.8322 - val_loss: 2562.7539\n",
      "Epoch 1286/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 92.0913 - val_loss: 2511.3196\n",
      "Epoch 1287/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 95.5009 - val_loss: 2463.3837\n",
      "Epoch 1288/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 89.1117 - val_loss: 2494.6863\n",
      "Epoch 1289/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 103.5562 - val_loss: 2542.6383\n",
      "Epoch 1290/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 103.0030 - val_loss: 2423.6141\n",
      "Epoch 1291/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 101.3604 - val_loss: 2603.1334\n",
      "Epoch 1292/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 113.3378 - val_loss: 2348.5576\n",
      "Epoch 1293/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 105.5133 - val_loss: 2426.9569\n",
      "Epoch 1294/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 108.3841 - val_loss: 2599.4353\n",
      "Epoch 1295/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 100.5844 - val_loss: 2366.0002\n",
      "Epoch 1296/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 87.3954 - val_loss: 2500.9381\n",
      "Epoch 1297/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 103.6939 - val_loss: 2515.6989\n",
      "Epoch 1298/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 97.6549 - val_loss: 2617.9503\n",
      "Epoch 1299/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 135.8349 - val_loss: 2527.5863\n",
      "Epoch 1300/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 131.2508 - val_loss: 2389.5585\n",
      "Epoch 1301/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 101.9534 - val_loss: 2554.5956\n",
      "Epoch 1302/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 96.6348 - val_loss: 2246.0154\n",
      "Epoch 1303/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 107.7823 - val_loss: 2519.9359\n",
      "Epoch 1304/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 106.2532 - val_loss: 2470.9858\n",
      "Epoch 1305/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 101.7818 - val_loss: 2656.5499\n",
      "Epoch 1306/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 91.7131 - val_loss: 2641.3017\n",
      "Epoch 1307/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 99.6821 - val_loss: 2457.7683\n",
      "Epoch 1308/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 100.8667 - val_loss: 2390.1518\n",
      "Epoch 1309/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 87.5865 - val_loss: 2454.3876\n",
      "Epoch 1310/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 80.2478 - val_loss: 2411.1761\n",
      "Epoch 1311/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 99.9757 - val_loss: 2530.5942\n",
      "Epoch 1312/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 98.6297 - val_loss: 2380.4567\n",
      "Epoch 1313/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 92.5295 - val_loss: 2624.7066\n",
      "Epoch 1314/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 110.8235 - val_loss: 2381.4887\n",
      "Epoch 1315/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 112.9765 - val_loss: 2653.8890\n",
      "Epoch 1316/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 123.3172 - val_loss: 2358.2844\n",
      "Epoch 1317/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 146.3514 - val_loss: 2534.5386\n",
      "Epoch 1318/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 238.5204 - val_loss: 2270.5460\n",
      "Epoch 1319/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 465.7035 - val_loss: 2583.1240\n",
      "Epoch 1320/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 628.0773 - val_loss: 1791.9932\n",
      "Epoch 1321/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1063.8269 - val_loss: 2596.2324\n",
      "Epoch 1322/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 764.0274 - val_loss: 3171.3991\n",
      "Epoch 1323/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2415.3331 - val_loss: 4462.6610\n",
      "Epoch 1324/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 4955.8001 - val_loss: 6870.4017\n",
      "Epoch 1325/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 5734.0295 - val_loss: 7385.5690\n",
      "Epoch 1326/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 3640.9675 - val_loss: 4588.5305\n",
      "Epoch 1327/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 3417.3512 - val_loss: 3895.9709\n",
      "Epoch 1328/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2771.2365 - val_loss: 4859.3657\n",
      "Epoch 1329/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 3102.0464 - val_loss: 4103.0720\n",
      "Epoch 1330/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2514.1183 - val_loss: 3690.0208\n",
      "Epoch 1331/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2687.8642 - val_loss: 3639.5061\n",
      "Epoch 1332/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2577.9856 - val_loss: 4034.9331\n",
      "Epoch 1333/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2684.3081 - val_loss: 3791.8346\n",
      "Epoch 1334/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2567.5018 - val_loss: 3625.4247\n",
      "Epoch 1335/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2584.5827 - val_loss: 3683.5568\n",
      "Epoch 1336/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2568.2528 - val_loss: 3781.1050\n",
      "Epoch 1337/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2542.7235 - val_loss: 3729.1719\n",
      "Epoch 1338/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2543.0307 - val_loss: 3639.7894\n",
      "Epoch 1339/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2517.7980 - val_loss: 3658.1119\n",
      "Epoch 1340/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2500.7306 - val_loss: 3752.9015\n",
      "Epoch 1341/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2508.9128 - val_loss: 3780.9202\n",
      "Epoch 1342/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 2555.9258 - val_loss: 3636.5754\n",
      "Epoch 1343/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2504.6187 - val_loss: 3701.2377\n",
      "Epoch 1344/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2471.9406 - val_loss: 3740.3887\n",
      "Epoch 1345/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2483.7926 - val_loss: 3677.7928\n",
      "Epoch 1346/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2429.3848 - val_loss: 3658.5351\n",
      "Epoch 1347/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2445.7709 - val_loss: 3699.5641\n",
      "Epoch 1348/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2470.0423 - val_loss: 3754.2608\n",
      "Epoch 1349/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2459.3892 - val_loss: 3751.5012\n",
      "Epoch 1350/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2439.0903 - val_loss: 3645.0552\n",
      "Epoch 1351/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2422.6344 - val_loss: 3688.7359\n",
      "Epoch 1352/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2418.4222 - val_loss: 3766.5948\n",
      "Epoch 1353/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2431.0115 - val_loss: 3723.5397\n",
      "Epoch 1354/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2396.4977 - val_loss: 3662.0418\n",
      "Epoch 1355/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2392.6860 - val_loss: 3671.3829\n",
      "Epoch 1356/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2376.7031 - val_loss: 3747.0899\n",
      "Epoch 1357/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2386.9855 - val_loss: 3804.3458\n",
      "Epoch 1358/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2352.9062 - val_loss: 3693.5855\n",
      "Epoch 1359/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2392.1662 - val_loss: 3655.8118\n",
      "Epoch 1360/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2362.1127 - val_loss: 3722.7611\n",
      "Epoch 1361/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2335.8743 - val_loss: 3836.7756\n",
      "Epoch 1362/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2379.1884 - val_loss: 3739.2510\n",
      "Epoch 1363/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2383.7969 - val_loss: 3661.1646\n",
      "Epoch 1364/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2313.8973 - val_loss: 3714.5009\n",
      "Epoch 1365/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2349.9253 - val_loss: 3733.8484\n",
      "Epoch 1366/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2289.9290 - val_loss: 3739.1157\n",
      "Epoch 1367/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2288.2594 - val_loss: 3701.5340\n",
      "Epoch 1368/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2299.8159 - val_loss: 3709.6186\n",
      "Epoch 1369/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2339.7095 - val_loss: 3720.9285\n",
      "Epoch 1370/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2346.2214 - val_loss: 3836.4771\n",
      "Epoch 1371/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2308.6767 - val_loss: 3703.8536\n",
      "Epoch 1372/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2381.9241 - val_loss: 3672.7934\n",
      "Epoch 1373/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2251.2731 - val_loss: 3804.9153\n",
      "Epoch 1374/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2349.9005 - val_loss: 3887.9839\n",
      "Epoch 1375/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2347.0190 - val_loss: 3682.9632\n",
      "Epoch 1376/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2318.1952 - val_loss: 3716.9781\n",
      "Epoch 1377/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2258.3966 - val_loss: 3728.5770\n",
      "Epoch 1378/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2248.9802 - val_loss: 3719.8894\n",
      "Epoch 1379/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2290.3474 - val_loss: 3704.5138\n",
      "Epoch 1380/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2286.3181 - val_loss: 3732.8787\n",
      "Epoch 1381/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2255.0672 - val_loss: 3719.7506\n",
      "Epoch 1382/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2281.5461 - val_loss: 3690.0603\n",
      "Epoch 1383/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2286.4383 - val_loss: 3754.3543\n",
      "Epoch 1384/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2249.8130 - val_loss: 3749.7866\n",
      "Epoch 1385/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2250.8815 - val_loss: 3678.6865\n",
      "Epoch 1386/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2244.4667 - val_loss: 3695.6095\n",
      "Epoch 1387/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2290.4051 - val_loss: 3803.8780\n",
      "Epoch 1388/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2287.5777 - val_loss: 3658.8777\n",
      "Epoch 1389/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2211.9302 - val_loss: 3710.6136\n",
      "Epoch 1390/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2230.4350 - val_loss: 3750.8770\n",
      "Epoch 1391/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2229.2286 - val_loss: 3669.2709\n",
      "Epoch 1392/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2228.9758 - val_loss: 3710.3300\n",
      "Epoch 1393/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2239.5902 - val_loss: 3666.5166\n",
      "Epoch 1394/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2208.2614 - val_loss: 3666.5459\n",
      "Epoch 1395/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2210.8240 - val_loss: 3790.3203\n",
      "Epoch 1396/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2212.6728 - val_loss: 3663.3382\n",
      "Epoch 1397/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2183.4391 - val_loss: 3617.9506\n",
      "Epoch 1398/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2292.0460 - val_loss: 3620.6833\n",
      "Epoch 1399/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2319.2015 - val_loss: 3976.0852\n",
      "Epoch 1400/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2278.9030 - val_loss: 3663.7363\n",
      "Epoch 1401/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2192.5837 - val_loss: 3599.5711\n",
      "Epoch 1402/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2222.6960 - val_loss: 3653.2480\n",
      "Epoch 1403/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2178.7917 - val_loss: 3729.5212\n",
      "Epoch 1404/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2224.1406 - val_loss: 3644.6882\n",
      "Epoch 1405/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2244.0548 - val_loss: 3590.1495\n",
      "Epoch 1406/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2169.8810 - val_loss: 3753.9987\n",
      "Epoch 1407/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2175.9903 - val_loss: 3700.3919\n",
      "Epoch 1408/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2136.7603 - val_loss: 3584.1805\n",
      "Epoch 1409/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2153.4943 - val_loss: 3591.5150\n",
      "Epoch 1410/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2163.1099 - val_loss: 3630.7518\n",
      "Epoch 1411/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2114.0230 - val_loss: 3625.8902\n",
      "Epoch 1412/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2150.8047 - val_loss: 3580.8818\n",
      "Epoch 1413/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2146.9313 - val_loss: 3593.6406\n",
      "Epoch 1414/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2127.4273 - val_loss: 3611.9083\n",
      "Epoch 1415/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2119.6762 - val_loss: 3595.8608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1416/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2118.3005 - val_loss: 3552.7452\n",
      "Epoch 1417/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2104.7129 - val_loss: 3569.5524\n",
      "Epoch 1418/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2115.3033 - val_loss: 3606.1701\n",
      "Epoch 1419/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2088.3550 - val_loss: 3541.4544\n",
      "Epoch 1420/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2089.2784 - val_loss: 3526.5471\n",
      "Epoch 1421/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2120.0081 - val_loss: 3577.9480\n",
      "Epoch 1422/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2094.6440 - val_loss: 3581.6680\n",
      "Epoch 1423/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2071.8105 - val_loss: 3488.4741\n",
      "Epoch 1424/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2075.7996 - val_loss: 3531.1861\n",
      "Epoch 1425/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2091.3130 - val_loss: 3566.8921\n",
      "Epoch 1426/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2093.5501 - val_loss: 3485.0887\n",
      "Epoch 1427/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2113.2497 - val_loss: 3459.7364\n",
      "Epoch 1428/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2073.5905 - val_loss: 3673.7936\n",
      "Epoch 1429/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2064.6362 - val_loss: 3477.3732\n",
      "Epoch 1430/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2169.8477 - val_loss: 3423.6126\n",
      "Epoch 1431/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2066.0086 - val_loss: 3599.2053\n",
      "Epoch 1432/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2049.0176 - val_loss: 3542.5438\n",
      "Epoch 1433/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2010.3935 - val_loss: 3426.1843\n",
      "Epoch 1434/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2086.7691 - val_loss: 3388.7247\n",
      "Epoch 1435/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2034.3263 - val_loss: 3559.1303\n",
      "Epoch 1436/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2062.2473 - val_loss: 3433.9095\n",
      "Epoch 1437/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2032.9613 - val_loss: 3380.2200\n",
      "Epoch 1438/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1977.2162 - val_loss: 3460.3220\n",
      "Epoch 1439/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1997.8578 - val_loss: 3439.2813\n",
      "Epoch 1440/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2002.2693 - val_loss: 3382.3351\n",
      "Epoch 1441/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1988.1344 - val_loss: 3347.5198\n",
      "Epoch 1442/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2015.1345 - val_loss: 3352.5062\n",
      "Epoch 1443/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1950.5941 - val_loss: 3405.7653\n",
      "Epoch 1444/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1933.4387 - val_loss: 3351.5687\n",
      "Epoch 1445/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1903.2072 - val_loss: 3320.2585\n",
      "Epoch 1446/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1923.7164 - val_loss: 3348.0156\n",
      "Epoch 1447/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1952.7424 - val_loss: 3325.6777\n",
      "Epoch 1448/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1915.2382 - val_loss: 3375.3879\n",
      "Epoch 1449/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1955.8140 - val_loss: 3275.7963\n",
      "Epoch 1450/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1899.9793 - val_loss: 3299.8381\n",
      "Epoch 1451/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1948.5544 - val_loss: 3330.8556\n",
      "Epoch 1452/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1929.4460 - val_loss: 3298.4452\n",
      "Epoch 1453/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1894.3337 - val_loss: 3235.3286\n",
      "Epoch 1454/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1884.3945 - val_loss: 3232.2142\n",
      "Epoch 1455/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1925.2065 - val_loss: 3291.5393\n",
      "Epoch 1456/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1930.4328 - val_loss: 3223.5138\n",
      "Epoch 1457/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1860.0519 - val_loss: 3193.3366\n",
      "Epoch 1458/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1909.5830 - val_loss: 3162.6898\n",
      "Epoch 1459/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1842.6002 - val_loss: 3279.9239\n",
      "Epoch 1460/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1869.1532 - val_loss: 3156.6635\n",
      "Epoch 1461/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1848.8040 - val_loss: 3160.5732\n",
      "Epoch 1462/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1838.6154 - val_loss: 3156.1965\n",
      "Epoch 1463/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1875.6155 - val_loss: 3241.7225\n",
      "Epoch 1464/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1821.5511 - val_loss: 3102.4402\n",
      "Epoch 1465/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1810.3873 - val_loss: 3098.5338\n",
      "Epoch 1466/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1803.5135 - val_loss: 3160.8961\n",
      "Epoch 1467/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1764.4363 - val_loss: 3128.8835\n",
      "Epoch 1468/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1831.4004 - val_loss: 3059.0864\n",
      "Epoch 1469/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1847.3709 - val_loss: 3189.2916\n",
      "Epoch 1470/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1752.3755 - val_loss: 3059.3262\n",
      "Epoch 1471/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1794.7372 - val_loss: 3060.3203\n",
      "Epoch 1472/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1831.5545 - val_loss: 3061.3835\n",
      "Epoch 1473/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1756.0799 - val_loss: 3113.4411\n",
      "Epoch 1474/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1749.7016 - val_loss: 2995.5461\n",
      "Epoch 1475/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1825.9923 - val_loss: 3048.8468\n",
      "Epoch 1476/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1815.4066 - val_loss: 3031.1793\n",
      "Epoch 1477/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1743.5924 - val_loss: 3069.1487\n",
      "Epoch 1478/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1731.0983 - val_loss: 2997.9352\n",
      "Epoch 1479/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1818.6478 - val_loss: 3001.9162\n",
      "Epoch 1480/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1806.6656 - val_loss: 3068.4970\n",
      "Epoch 1481/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1769.3753 - val_loss: 2928.2097\n",
      "Epoch 1482/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1757.2721 - val_loss: 3030.5924\n",
      "Epoch 1483/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1798.0431 - val_loss: 3019.1137\n",
      "Epoch 1484/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1797.8133 - val_loss: 2912.4110\n",
      "Epoch 1485/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1790.9389 - val_loss: 3165.3513\n",
      "Epoch 1486/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1753.7256 - val_loss: 2897.2544\n",
      "Epoch 1487/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1729.4832 - val_loss: 2934.0851\n",
      "Epoch 1488/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1732.3287 - val_loss: 2938.6912\n",
      "Epoch 1489/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1678.3971 - val_loss: 2884.5639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1490/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1716.9844 - val_loss: 2960.7411\n",
      "Epoch 1491/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1678.4891 - val_loss: 2912.1820\n",
      "Epoch 1492/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1640.7134 - val_loss: 2861.2283\n",
      "Epoch 1493/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1729.9727 - val_loss: 2917.1305\n",
      "Epoch 1494/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1632.3328 - val_loss: 2858.7967\n",
      "Epoch 1495/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1725.4046 - val_loss: 2940.5463\n",
      "Epoch 1496/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1729.4971 - val_loss: 2908.0924\n",
      "Epoch 1497/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1697.8036 - val_loss: 2819.8688\n",
      "Epoch 1498/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1680.4550 - val_loss: 2891.8924\n",
      "Epoch 1499/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1650.8985 - val_loss: 2863.3167\n",
      "Epoch 1500/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1651.9702 - val_loss: 2807.4281\n",
      "Epoch 1501/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1662.3077 - val_loss: 2900.2150\n",
      "Epoch 1502/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1682.2536 - val_loss: 2894.0797\n",
      "Epoch 1503/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1619.2555 - val_loss: 2787.9752\n",
      "Epoch 1504/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1594.6852 - val_loss: 2966.2326\n",
      "Epoch 1505/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1722.0318 - val_loss: 2869.5078\n",
      "Epoch 1506/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1559.6261 - val_loss: 2772.2529\n",
      "Epoch 1507/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1731.8853 - val_loss: 2833.1048\n",
      "Epoch 1508/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 2014.5068 - val_loss: 3008.9354\n",
      "Epoch 1509/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1954.7145 - val_loss: 2804.9744\n",
      "Epoch 1510/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1674.2155 - val_loss: 3053.9307\n",
      "Epoch 1511/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1720.8832 - val_loss: 2773.8081\n",
      "Epoch 1512/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1646.5923 - val_loss: 2799.7806\n",
      "Epoch 1513/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1591.4619 - val_loss: 2832.4255\n",
      "Epoch 1514/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1604.5523 - val_loss: 2790.5843\n",
      "Epoch 1515/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1600.8482 - val_loss: 2817.5031\n",
      "Epoch 1516/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1612.7212 - val_loss: 2794.7619\n",
      "Epoch 1517/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1529.3501 - val_loss: 2738.0946\n",
      "Epoch 1518/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1700.9479 - val_loss: 2832.7519\n",
      "Epoch 1519/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1559.8225 - val_loss: 2736.2349\n",
      "Epoch 1520/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1558.3949 - val_loss: 2804.7263\n",
      "Epoch 1521/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1589.6157 - val_loss: 2771.0217\n",
      "Epoch 1522/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1592.9577 - val_loss: 2742.2292\n",
      "Epoch 1523/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1643.7637 - val_loss: 2786.4081\n",
      "Epoch 1524/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1569.3275 - val_loss: 2944.5163\n",
      "Epoch 1525/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1545.5423 - val_loss: 2708.3266\n",
      "Epoch 1526/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1595.1161 - val_loss: 2711.7608\n",
      "Epoch 1527/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1811.0384 - val_loss: 2918.3892\n",
      "Epoch 1528/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1544.2939 - val_loss: 2768.8116\n",
      "Epoch 1529/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1654.0077 - val_loss: 2932.1866\n",
      "Epoch 1530/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1599.6981 - val_loss: 2720.6493\n",
      "Epoch 1531/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1506.7726 - val_loss: 2754.2943\n",
      "Epoch 1532/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1479.4909 - val_loss: 2694.9579\n",
      "Epoch 1533/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1599.7913 - val_loss: 2716.2825\n",
      "Epoch 1534/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1539.7475 - val_loss: 2686.3875\n",
      "Epoch 1535/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1471.7146 - val_loss: 2781.4990\n",
      "Epoch 1536/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1562.6428 - val_loss: 2785.8487\n",
      "Epoch 1537/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1499.7275 - val_loss: 2676.3748\n",
      "Epoch 1538/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1585.4865 - val_loss: 2725.4268\n",
      "Epoch 1539/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1451.0669 - val_loss: 2997.9271\n",
      "Epoch 1540/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1553.9705 - val_loss: 2675.4442\n",
      "Epoch 1541/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1530.9032 - val_loss: 2694.3416\n",
      "Epoch 1542/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1581.9524 - val_loss: 2835.7141\n",
      "Epoch 1543/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1533.7295 - val_loss: 2672.2794\n",
      "Epoch 1544/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1532.3499 - val_loss: 2744.8194\n",
      "Epoch 1545/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1624.1846 - val_loss: 2708.9418\n",
      "Epoch 1546/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1518.0504 - val_loss: 2662.1522\n",
      "Epoch 1547/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1533.6829 - val_loss: 2836.0005\n",
      "Epoch 1548/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1516.9831 - val_loss: 2690.1268\n",
      "Epoch 1549/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1460.2634 - val_loss: 2674.4781\n",
      "Epoch 1550/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1416.0028 - val_loss: 2776.0964\n",
      "Epoch 1551/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1453.2707 - val_loss: 2697.4558\n",
      "Epoch 1552/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1498.5308 - val_loss: 2680.0020\n",
      "Epoch 1553/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1467.5855 - val_loss: 2696.0255\n",
      "Epoch 1554/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1485.7927 - val_loss: 2714.6374\n",
      "Epoch 1555/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1413.6163 - val_loss: 2727.3015\n",
      "Epoch 1556/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1450.5842 - val_loss: 2678.8937\n",
      "Epoch 1557/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1422.7122 - val_loss: 2691.0428\n",
      "Epoch 1558/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1469.2506 - val_loss: 2693.6198\n",
      "Epoch 1559/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1456.1194 - val_loss: 2710.4871\n",
      "Epoch 1560/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1455.9470 - val_loss: 2702.1691\n",
      "Epoch 1561/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1457.2075 - val_loss: 2703.6991\n",
      "Epoch 1562/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1431.1554 - val_loss: 2696.3803\n",
      "Epoch 1563/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1381.7960 - val_loss: 2754.3497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1564/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1463.2013 - val_loss: 2673.8926\n",
      "Epoch 1565/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1481.2845 - val_loss: 2721.3567\n",
      "Epoch 1566/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1442.3679 - val_loss: 2696.1189\n",
      "Epoch 1567/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1383.7211 - val_loss: 2666.4038\n",
      "Epoch 1568/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1408.8119 - val_loss: 2734.6672\n",
      "Epoch 1569/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1463.3358 - val_loss: 2722.9816\n",
      "Epoch 1570/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1468.2230 - val_loss: 2717.1685\n",
      "Epoch 1571/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1307.2389 - val_loss: 2665.4641\n",
      "Epoch 1572/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1415.0375 - val_loss: 2834.3909\n",
      "Epoch 1573/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1388.0565 - val_loss: 2665.3287\n",
      "Epoch 1574/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1430.2316 - val_loss: 2673.2280\n",
      "Epoch 1575/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1352.0376 - val_loss: 2865.9101\n",
      "Epoch 1576/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1416.1324 - val_loss: 2659.7503\n",
      "Epoch 1577/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1525.4382 - val_loss: 2674.1970\n",
      "Epoch 1578/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1459.1300 - val_loss: 2936.6991\n",
      "Epoch 1579/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1419.8262 - val_loss: 2690.0510\n",
      "Epoch 1580/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1418.3131 - val_loss: 2734.1052\n",
      "Epoch 1581/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1467.6597 - val_loss: 2771.9391\n",
      "Epoch 1582/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1310.2078 - val_loss: 2662.3716\n",
      "Epoch 1583/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1368.4291 - val_loss: 2689.4185\n",
      "Epoch 1584/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1275.0060 - val_loss: 2685.3051\n",
      "Epoch 1585/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1328.4144 - val_loss: 2694.9073\n",
      "Epoch 1586/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1331.1373 - val_loss: 2719.2328\n",
      "Epoch 1587/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1333.9103 - val_loss: 2707.8478\n",
      "Epoch 1588/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1376.0705 - val_loss: 2665.0360\n",
      "Epoch 1589/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1354.8296 - val_loss: 2795.2255\n",
      "Epoch 1590/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1308.4934 - val_loss: 2668.5772\n",
      "Epoch 1591/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1379.3448 - val_loss: 2766.0693\n",
      "Epoch 1592/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1318.6026 - val_loss: 2669.5987\n",
      "Epoch 1593/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1334.8992 - val_loss: 2781.2482\n",
      "Epoch 1594/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1391.1081 - val_loss: 2698.8972\n",
      "Epoch 1595/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1465.7063 - val_loss: 2683.6898\n",
      "Epoch 1596/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1354.4614 - val_loss: 3050.7917\n",
      "Epoch 1597/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1422.3525 - val_loss: 2664.1660\n",
      "Epoch 1598/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1360.3874 - val_loss: 2795.3086\n",
      "Epoch 1599/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1297.7970 - val_loss: 2687.8628\n",
      "Epoch 1600/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1215.3026 - val_loss: 2659.9303\n",
      "Epoch 1601/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1269.6097 - val_loss: 2689.5527\n",
      "Epoch 1602/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1309.7710 - val_loss: 2681.6082\n",
      "Epoch 1603/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1258.8313 - val_loss: 2663.9558\n",
      "Epoch 1604/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1314.7243 - val_loss: 2716.0436\n",
      "Epoch 1605/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1310.7602 - val_loss: 2688.2419\n",
      "Epoch 1606/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1285.0458 - val_loss: 2735.5240\n",
      "Epoch 1607/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1265.4714 - val_loss: 2663.5613\n",
      "Epoch 1608/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1217.8665 - val_loss: 2847.7286\n",
      "Epoch 1609/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1260.4290 - val_loss: 2685.7489\n",
      "Epoch 1610/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1217.5139 - val_loss: 2678.1304\n",
      "Epoch 1611/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1221.4039 - val_loss: 2728.1539\n",
      "Epoch 1612/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1217.7299 - val_loss: 2715.3128\n",
      "Epoch 1613/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1208.3115 - val_loss: 2692.4564\n",
      "Epoch 1614/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1226.1211 - val_loss: 2737.9842\n",
      "Epoch 1615/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1192.3274 - val_loss: 2657.0202\n",
      "Epoch 1616/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1288.9257 - val_loss: 2755.1708\n",
      "Epoch 1617/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1257.6564 - val_loss: 2671.1994\n",
      "Epoch 1618/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1243.4502 - val_loss: 2669.2280\n",
      "Epoch 1619/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1171.3306 - val_loss: 2902.8416\n",
      "Epoch 1620/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1317.2844 - val_loss: 2668.1462\n",
      "Epoch 1621/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1212.9044 - val_loss: 2653.8863\n",
      "Epoch 1622/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1203.8481 - val_loss: 2772.6605\n",
      "Epoch 1623/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1247.6076 - val_loss: 2644.3799\n",
      "Epoch 1624/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1225.6571 - val_loss: 2728.0222\n",
      "Epoch 1625/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1221.1965 - val_loss: 2739.4296\n",
      "Epoch 1626/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1178.7518 - val_loss: 2640.8684\n",
      "Epoch 1627/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1286.4604 - val_loss: 2784.6333\n",
      "Epoch 1628/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1328.1606 - val_loss: 2640.4781\n",
      "Epoch 1629/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1136.5032 - val_loss: 2884.7440\n",
      "Epoch 1630/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1200.4231 - val_loss: 2629.3504\n",
      "Epoch 1631/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1204.2915 - val_loss: 2669.0760\n",
      "Epoch 1632/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1149.2415 - val_loss: 2652.2768\n",
      "Epoch 1633/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1112.7218 - val_loss: 2672.0834\n",
      "Epoch 1634/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1146.2731 - val_loss: 2644.6910\n",
      "Epoch 1635/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1141.3972 - val_loss: 2659.3425\n",
      "Epoch 1636/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1126.5827 - val_loss: 2716.1049\n",
      "Epoch 1637/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1133.1125 - val_loss: 2628.9189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1638/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1069.4642 - val_loss: 2706.8719\n",
      "Epoch 1639/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1104.1514 - val_loss: 2640.9513\n",
      "Epoch 1640/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1084.0176 - val_loss: 2685.9942\n",
      "Epoch 1641/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1073.3977 - val_loss: 2627.4326\n",
      "Epoch 1642/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1058.6060 - val_loss: 2631.0745\n",
      "Epoch 1643/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1113.5036 - val_loss: 2678.4631\n",
      "Epoch 1644/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1078.4002 - val_loss: 2617.8920\n",
      "Epoch 1645/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1066.3606 - val_loss: 2716.8284\n",
      "Epoch 1646/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1063.5265 - val_loss: 2634.0466\n",
      "Epoch 1647/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1096.9703 - val_loss: 2691.2322\n",
      "Epoch 1648/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1094.9143 - val_loss: 2608.6455\n",
      "Epoch 1649/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1072.4344 - val_loss: 2897.5802\n",
      "Epoch 1650/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1171.2949 - val_loss: 2606.3891\n",
      "Epoch 1651/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1082.1764 - val_loss: 2748.6926\n",
      "Epoch 1652/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1017.0290 - val_loss: 2610.6832\n",
      "Epoch 1653/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1150.1669 - val_loss: 2787.9986\n",
      "Epoch 1654/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1294.9614 - val_loss: 2635.2841\n",
      "Epoch 1655/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1171.6223 - val_loss: 2590.9229\n",
      "Epoch 1656/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1051.5886 - val_loss: 2876.1158\n",
      "Epoch 1657/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1047.6490 - val_loss: 2582.7405\n",
      "Epoch 1658/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1044.2021 - val_loss: 2721.3554\n",
      "Epoch 1659/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1090.5506 - val_loss: 2640.8427\n",
      "Epoch 1660/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 993.8822 - val_loss: 2595.0409\n",
      "Epoch 1661/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1017.4085 - val_loss: 2683.2558\n",
      "Epoch 1662/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1048.7025 - val_loss: 2622.6059\n",
      "Epoch 1663/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1050.1805 - val_loss: 2688.8907\n",
      "Epoch 1664/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 973.6737 - val_loss: 2622.8857\n",
      "Epoch 1665/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1002.8359 - val_loss: 2882.4217\n",
      "Epoch 1666/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1023.3958 - val_loss: 2613.6346\n",
      "Epoch 1667/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 985.4507 - val_loss: 2727.8259\n",
      "Epoch 1668/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1026.0342 - val_loss: 2626.1069\n",
      "Epoch 1669/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1050.4956 - val_loss: 2730.2568\n",
      "Epoch 1670/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 987.4356 - val_loss: 2640.6073\n",
      "Epoch 1671/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 954.8686 - val_loss: 2719.8693\n",
      "Epoch 1672/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 931.9521 - val_loss: 2664.8586\n",
      "Epoch 1673/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 947.7248 - val_loss: 2803.9662\n",
      "Epoch 1674/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 1012.3909 - val_loss: 2696.2752\n",
      "Epoch 1675/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 964.6340 - val_loss: 2880.7595\n",
      "Epoch 1676/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 929.4020 - val_loss: 2729.3197\n",
      "Epoch 1677/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 896.5177 - val_loss: 2780.8853\n",
      "Epoch 1678/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 937.0055 - val_loss: 2809.8055\n",
      "Epoch 1679/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 916.4812 - val_loss: 2756.5157\n",
      "Epoch 1680/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 916.3012 - val_loss: 2834.0394\n",
      "Epoch 1681/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 900.0213 - val_loss: 2753.1419\n",
      "Epoch 1682/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 914.3342 - val_loss: 2818.2244\n",
      "Epoch 1683/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 878.9728 - val_loss: 2730.9661\n",
      "Epoch 1684/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 885.0160 - val_loss: 2970.7881\n",
      "Epoch 1685/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 966.9927 - val_loss: 2771.3154\n",
      "Epoch 1686/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 816.0981 - val_loss: 2825.6559\n",
      "Epoch 1687/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 861.4251 - val_loss: 2812.0436\n",
      "Epoch 1688/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 868.1350 - val_loss: 2785.8810\n",
      "Epoch 1689/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 816.3629 - val_loss: 2852.0476\n",
      "Epoch 1690/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 824.9040 - val_loss: 2861.2729\n",
      "Epoch 1691/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 864.2697 - val_loss: 2865.3057\n",
      "Epoch 1692/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 848.4480 - val_loss: 2860.0247\n",
      "Epoch 1693/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 865.7694 - val_loss: 2852.0744\n",
      "Epoch 1694/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 839.1628 - val_loss: 2817.4652\n",
      "Epoch 1695/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 855.2024 - val_loss: 2956.2641\n",
      "Epoch 1696/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 953.2224 - val_loss: 2825.0108\n",
      "Epoch 1697/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 898.9949 - val_loss: 2841.6145\n",
      "Epoch 1698/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 855.5093 - val_loss: 2876.0371\n",
      "Epoch 1699/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 861.8183 - val_loss: 2812.7802\n",
      "Epoch 1700/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 799.3313 - val_loss: 3043.2924\n",
      "Epoch 1701/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 828.5098 - val_loss: 2838.4490\n",
      "Epoch 1702/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 823.4046 - val_loss: 2960.3828\n",
      "Epoch 1703/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 932.9609 - val_loss: 2812.1729\n",
      "Epoch 1704/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1091.2667 - val_loss: 2835.1947\n",
      "Epoch 1705/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1052.0189 - val_loss: 3042.1149\n",
      "Epoch 1706/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1006.2427 - val_loss: 2828.1400\n",
      "Epoch 1707/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 947.6767 - val_loss: 3086.2990\n",
      "Epoch 1708/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 887.0104 - val_loss: 2788.0840\n",
      "Epoch 1709/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 819.4659 - val_loss: 2941.1821\n",
      "Epoch 1710/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 873.2220 - val_loss: 2751.8711\n",
      "Epoch 1711/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 903.8503 - val_loss: 2799.1760\n",
      "Epoch 1712/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 2ms/step - loss: 867.5673 - val_loss: 2934.1569\n",
      "Epoch 1713/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 858.4976 - val_loss: 2780.4569\n",
      "Epoch 1714/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 806.0752 - val_loss: 3034.7289\n",
      "Epoch 1715/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 824.1850 - val_loss: 2767.0769\n",
      "Epoch 1716/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 893.5307 - val_loss: 2845.5422\n",
      "Epoch 1717/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 847.9880 - val_loss: 2994.5294\n",
      "Epoch 1718/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 808.6639 - val_loss: 2804.1664\n",
      "Epoch 1719/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 774.2986 - val_loss: 2978.8494\n",
      "Epoch 1720/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 793.7419 - val_loss: 2792.8410\n",
      "Epoch 1721/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 851.2713 - val_loss: 2930.2232\n",
      "Epoch 1722/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 776.0201 - val_loss: 2823.7459\n",
      "Epoch 1723/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 817.1700 - val_loss: 2859.0003\n",
      "Epoch 1724/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 836.2144 - val_loss: 2928.0392\n",
      "Epoch 1725/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 795.9580 - val_loss: 2794.4651\n",
      "Epoch 1726/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 829.5514 - val_loss: 3097.3186\n",
      "Epoch 1727/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 758.5401 - val_loss: 2825.5549\n",
      "Epoch 1728/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 820.6182 - val_loss: 2916.6023\n",
      "Epoch 1729/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 772.6621 - val_loss: 2903.4502\n",
      "Epoch 1730/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 826.5141 - val_loss: 2870.8391\n",
      "Epoch 1731/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 760.6871 - val_loss: 2943.0508\n",
      "Epoch 1732/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 740.7733 - val_loss: 2834.3402\n",
      "Epoch 1733/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 745.2553 - val_loss: 2887.7975\n",
      "Epoch 1734/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 782.9970 - val_loss: 2939.3405\n",
      "Epoch 1735/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 711.0209 - val_loss: 2860.9227\n",
      "Epoch 1736/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 733.8946 - val_loss: 2932.8952\n",
      "Epoch 1737/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 797.9580 - val_loss: 2858.2146\n",
      "Epoch 1738/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 756.0765 - val_loss: 2998.6094\n",
      "Epoch 1739/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 767.1321 - val_loss: 2872.9455\n",
      "Epoch 1740/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 757.2976 - val_loss: 2835.3547\n",
      "Epoch 1741/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 796.2329 - val_loss: 2893.2332\n",
      "Epoch 1742/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 756.5977 - val_loss: 2893.7256\n",
      "Epoch 1743/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 743.8267 - val_loss: 3106.8379\n",
      "Epoch 1744/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 703.2040 - val_loss: 2843.2204\n",
      "Epoch 1745/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 747.4053 - val_loss: 2900.0027\n",
      "Epoch 1746/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 748.6393 - val_loss: 2873.3561\n",
      "Epoch 1747/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 791.0918 - val_loss: 2857.8791\n",
      "Epoch 1748/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 800.4751 - val_loss: 2981.4335\n",
      "Epoch 1749/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 793.2367 - val_loss: 2826.2141\n",
      "Epoch 1750/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 871.1365 - val_loss: 2972.9971\n",
      "Epoch 1751/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 808.7115 - val_loss: 2824.1414\n",
      "Epoch 1752/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 737.2465 - val_loss: 3081.6407\n",
      "Epoch 1753/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 718.7209 - val_loss: 2849.9058\n",
      "Epoch 1754/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 775.1736 - val_loss: 2890.1970\n",
      "Epoch 1755/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 795.9935 - val_loss: 2914.6177\n",
      "Epoch 1756/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 709.7410 - val_loss: 2849.5424\n",
      "Epoch 1757/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 718.6993 - val_loss: 3033.5301\n",
      "Epoch 1758/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 762.5497 - val_loss: 2884.8841\n",
      "Epoch 1759/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 737.4655 - val_loss: 2915.6295\n",
      "Epoch 1760/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 668.2951 - val_loss: 2944.4850\n",
      "Epoch 1761/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 707.7854 - val_loss: 2862.2426\n",
      "Epoch 1762/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 680.6310 - val_loss: 2961.0512\n",
      "Epoch 1763/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 708.5629 - val_loss: 2876.1935\n",
      "Epoch 1764/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 675.1771 - val_loss: 2858.0325\n",
      "Epoch 1765/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 708.7327 - val_loss: 2884.2637\n",
      "Epoch 1766/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 714.8452 - val_loss: 2852.1781\n",
      "Epoch 1767/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 726.1456 - val_loss: 2876.2504\n",
      "Epoch 1768/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 686.4036 - val_loss: 3067.5116\n",
      "Epoch 1769/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 646.8963 - val_loss: 2860.6639\n",
      "Epoch 1770/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 716.1547 - val_loss: 2967.5784\n",
      "Epoch 1771/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 718.6677 - val_loss: 2878.2137\n",
      "Epoch 1772/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 796.9115 - val_loss: 2852.3464\n",
      "Epoch 1773/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 703.8516 - val_loss: 3033.3244\n",
      "Epoch 1774/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 660.5605 - val_loss: 2808.7797\n",
      "Epoch 1775/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 690.1060 - val_loss: 2904.2312\n",
      "Epoch 1776/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 697.6911 - val_loss: 2880.5758\n",
      "Epoch 1777/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 697.8920 - val_loss: 2807.2069\n",
      "Epoch 1778/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 728.7446 - val_loss: 2920.8519\n",
      "Epoch 1779/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 692.9662 - val_loss: 2859.4762\n",
      "Epoch 1780/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 681.8479 - val_loss: 2990.1750\n",
      "Epoch 1781/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 677.7673 - val_loss: 2941.3944\n",
      "Epoch 1782/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 756.5821 - val_loss: 2874.3327\n",
      "Epoch 1783/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 806.5044 - val_loss: 2888.7070\n",
      "Epoch 1784/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 664.5807 - val_loss: 2856.0273\n",
      "Epoch 1785/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 681.7017 - val_loss: 3127.3315\n",
      "Epoch 1786/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 2ms/step - loss: 693.4837 - val_loss: 2873.8465\n",
      "Epoch 1787/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 680.7084 - val_loss: 2932.5732\n",
      "Epoch 1788/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 649.7121 - val_loss: 2919.4836\n",
      "Epoch 1789/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 645.5276 - val_loss: 2882.9167\n",
      "Epoch 1790/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 684.2520 - val_loss: 3002.1603\n",
      "Epoch 1791/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 668.0464 - val_loss: 2862.4039\n",
      "Epoch 1792/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 665.7202 - val_loss: 2893.3696\n",
      "Epoch 1793/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 659.8418 - val_loss: 2910.2035\n",
      "Epoch 1794/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 630.9031 - val_loss: 2938.8589\n",
      "Epoch 1795/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 643.3409 - val_loss: 2985.8192\n",
      "Epoch 1796/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 661.8408 - val_loss: 2886.5111\n",
      "Epoch 1797/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 672.9285 - val_loss: 2932.3237\n",
      "Epoch 1798/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 645.0742 - val_loss: 2992.5824\n",
      "Epoch 1799/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 693.9463 - val_loss: 2889.9815\n",
      "Epoch 1800/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 778.1571 - val_loss: 3022.1296\n",
      "Epoch 1801/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 663.4602 - val_loss: 2888.7468\n",
      "Epoch 1802/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 736.4002 - val_loss: 3136.6773\n",
      "Epoch 1803/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 658.9795 - val_loss: 2850.8522\n",
      "Epoch 1804/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 640.7298 - val_loss: 2941.2269\n",
      "Epoch 1805/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 612.1286 - val_loss: 2888.8246\n",
      "Epoch 1806/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 629.8032 - val_loss: 2921.1266\n",
      "Epoch 1807/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 648.2180 - val_loss: 2919.2472\n",
      "Epoch 1808/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 658.3957 - val_loss: 2839.6237\n",
      "Epoch 1809/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 679.3848 - val_loss: 3000.5271\n",
      "Epoch 1810/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 636.7403 - val_loss: 2891.6985\n",
      "Epoch 1811/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 622.5443 - val_loss: 3171.9882\n",
      "Epoch 1812/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 652.9249 - val_loss: 2864.9520\n",
      "Epoch 1813/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 640.2901 - val_loss: 2886.0130\n",
      "Epoch 1814/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 642.8234 - val_loss: 2955.9479\n",
      "Epoch 1815/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 679.0845 - val_loss: 2882.0785\n",
      "Epoch 1816/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 610.8005 - val_loss: 3135.2109\n",
      "Epoch 1817/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 632.0174 - val_loss: 2865.1577\n",
      "Epoch 1818/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 641.3771 - val_loss: 2924.2596\n",
      "Epoch 1819/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 652.0171 - val_loss: 2875.0461\n",
      "Epoch 1820/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 585.1467 - val_loss: 2959.2773\n",
      "Epoch 1821/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 578.1180 - val_loss: 2876.4350\n",
      "Epoch 1822/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 635.4802 - val_loss: 2980.0858\n",
      "Epoch 1823/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 631.6982 - val_loss: 2931.8742\n",
      "Epoch 1824/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 607.1643 - val_loss: 2935.7729\n",
      "Epoch 1825/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 610.8431 - val_loss: 2939.4760\n",
      "Epoch 1826/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 578.2652 - val_loss: 2921.0183\n",
      "Epoch 1827/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 585.8849 - val_loss: 2996.1576\n",
      "Epoch 1828/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 620.2932 - val_loss: 2966.6685\n",
      "Epoch 1829/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 599.3002 - val_loss: 2966.0003\n",
      "Epoch 1830/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 607.7069 - val_loss: 2958.9241\n",
      "Epoch 1831/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 577.3564 - val_loss: 3075.1903\n",
      "Epoch 1832/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 602.6369 - val_loss: 2931.8837\n",
      "Epoch 1833/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 573.2025 - val_loss: 2970.2747\n",
      "Epoch 1834/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 586.3730 - val_loss: 2994.7488\n",
      "Epoch 1835/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 600.9092 - val_loss: 3011.1356\n",
      "Epoch 1836/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 528.6957 - val_loss: 3024.3684\n",
      "Epoch 1837/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 547.7276 - val_loss: 2955.3208\n",
      "Epoch 1838/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 564.9729 - val_loss: 2971.4817\n",
      "Epoch 1839/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 578.5860 - val_loss: 2983.7272\n",
      "Epoch 1840/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 569.7590 - val_loss: 3046.1069\n",
      "Epoch 1841/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 542.1026 - val_loss: 3041.3976\n",
      "Epoch 1842/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 582.1773 - val_loss: 2961.5680\n",
      "Epoch 1843/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 611.3503 - val_loss: 2961.9171\n",
      "Epoch 1844/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 590.2974 - val_loss: 2979.3793\n",
      "Epoch 1845/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 559.0465 - val_loss: 3083.4896\n",
      "Epoch 1846/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 560.1781 - val_loss: 2975.0453\n",
      "Epoch 1847/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 572.8322 - val_loss: 3040.3346\n",
      "Epoch 1848/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 557.3486 - val_loss: 3083.5505\n",
      "Epoch 1849/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 541.8680 - val_loss: 2956.4614\n",
      "Epoch 1850/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 610.4401 - val_loss: 3085.4657\n",
      "Epoch 1851/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 598.5445 - val_loss: 2982.7750\n",
      "Epoch 1852/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 603.3555 - val_loss: 2948.3078\n",
      "Epoch 1853/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 758.7682 - val_loss: 3072.3402\n",
      "Epoch 1854/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 683.9252 - val_loss: 2950.8841\n",
      "Epoch 1855/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 566.3238 - val_loss: 3352.9102\n",
      "Epoch 1856/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 604.9394 - val_loss: 2912.6948\n",
      "Epoch 1857/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 640.5598 - val_loss: 2996.8577\n",
      "Epoch 1858/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 665.9895 - val_loss: 2833.8964\n",
      "Epoch 1859/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 558.6349 - val_loss: 2918.3189\n",
      "Epoch 1860/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 1ms/step - loss: 559.7805 - val_loss: 3029.7921\n",
      "Epoch 1861/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 554.9418 - val_loss: 2884.4393\n",
      "Epoch 1862/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 570.1530 - val_loss: 2999.6637\n",
      "Epoch 1863/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 563.9304 - val_loss: 2971.0637\n",
      "Epoch 1864/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 555.7711 - val_loss: 3069.5396\n",
      "Epoch 1865/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 545.5156 - val_loss: 2982.8776\n",
      "Epoch 1866/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 549.8648 - val_loss: 2993.1415\n",
      "Epoch 1867/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 528.8787 - val_loss: 2981.6131\n",
      "Epoch 1868/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 537.0121 - val_loss: 2975.5363\n",
      "Epoch 1869/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 534.7640 - val_loss: 3078.4948\n",
      "Epoch 1870/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 555.8796 - val_loss: 2972.6900\n",
      "Epoch 1871/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 516.1425 - val_loss: 3114.8782\n",
      "Epoch 1872/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 576.8715 - val_loss: 3007.6441\n",
      "Epoch 1873/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 552.6032 - val_loss: 3150.8737\n",
      "Epoch 1874/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 492.6574 - val_loss: 2909.0138\n",
      "Epoch 1875/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 526.7962 - val_loss: 3005.9791\n",
      "Epoch 1876/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 522.6262 - val_loss: 3044.7752\n",
      "Epoch 1877/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 517.1605 - val_loss: 2985.9891\n",
      "Epoch 1878/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 535.8041 - val_loss: 2981.4752\n",
      "Epoch 1879/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 537.1845 - val_loss: 3064.6099\n",
      "Epoch 1880/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 518.0404 - val_loss: 3101.5749\n",
      "Epoch 1881/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 489.5602 - val_loss: 2949.7198\n",
      "Epoch 1882/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 493.8521 - val_loss: 3039.6079\n",
      "Epoch 1883/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 495.3975 - val_loss: 3046.7844\n",
      "Epoch 1884/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 503.0715 - val_loss: 3035.6638\n",
      "Epoch 1885/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 504.9678 - val_loss: 2960.2475\n",
      "Epoch 1886/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 500.5442 - val_loss: 3024.7191\n",
      "Epoch 1887/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 485.3910 - val_loss: 3010.2486\n",
      "Epoch 1888/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 472.6983 - val_loss: 3043.4724\n",
      "Epoch 1889/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 487.5514 - val_loss: 3132.5474\n",
      "Epoch 1890/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 459.7520 - val_loss: 2987.3910\n",
      "Epoch 1891/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 493.1018 - val_loss: 3129.0676\n",
      "Epoch 1892/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 511.3331 - val_loss: 2983.4649\n",
      "Epoch 1893/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 495.6696 - val_loss: 3042.1916\n",
      "Epoch 1894/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 436.7183 - val_loss: 2991.9095\n",
      "Epoch 1895/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 457.8541 - val_loss: 3007.5151\n",
      "Epoch 1896/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 454.7634 - val_loss: 3060.9969\n",
      "Epoch 1897/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 455.9763 - val_loss: 3033.8460\n",
      "Epoch 1898/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 504.7707 - val_loss: 3007.8137\n",
      "Epoch 1899/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 491.3543 - val_loss: 3076.0108\n",
      "Epoch 1900/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 500.1012 - val_loss: 3145.5660\n",
      "Epoch 1901/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 498.9372 - val_loss: 2930.6081\n",
      "Epoch 1902/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 561.8835 - val_loss: 3083.7015\n",
      "Epoch 1903/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 467.7115 - val_loss: 3104.1807\n",
      "Epoch 1904/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 456.3004 - val_loss: 3046.8421\n",
      "Epoch 1905/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 466.7023 - val_loss: 3016.9764\n",
      "Epoch 1906/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 441.8675 - val_loss: 3112.2558\n",
      "Epoch 1907/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 417.6818 - val_loss: 3056.9376\n",
      "Epoch 1908/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 442.8975 - val_loss: 3019.5801\n",
      "Epoch 1909/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 411.8601 - val_loss: 3077.3121\n",
      "Epoch 1910/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 439.6283 - val_loss: 3118.1422\n",
      "Epoch 1911/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 450.7204 - val_loss: 3055.4423\n",
      "Epoch 1912/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 425.4109 - val_loss: 3107.3821\n",
      "Epoch 1913/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 423.7269 - val_loss: 3186.6774\n",
      "Epoch 1914/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 481.9144 - val_loss: 3015.2329\n",
      "Epoch 1915/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 465.9012 - val_loss: 3252.5035\n",
      "Epoch 1916/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 530.8839 - val_loss: 3109.5752\n",
      "Epoch 1917/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 474.6739 - val_loss: 3199.0187\n",
      "Epoch 1918/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 527.4296 - val_loss: 3066.0507\n",
      "Epoch 1919/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 491.0804 - val_loss: 3582.3609\n",
      "Epoch 1920/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 463.4043 - val_loss: 3068.9274\n",
      "Epoch 1921/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 459.3489 - val_loss: 3093.8896\n",
      "Epoch 1922/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 411.9418 - val_loss: 3050.7139\n",
      "Epoch 1923/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 398.2725 - val_loss: 3173.2276\n",
      "Epoch 1924/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 408.1669 - val_loss: 3042.9950\n",
      "Epoch 1925/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 428.4516 - val_loss: 3059.2223\n",
      "Epoch 1926/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 410.7405 - val_loss: 3039.5111\n",
      "Epoch 1927/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 425.5503 - val_loss: 3131.4905\n",
      "Epoch 1928/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 384.7743 - val_loss: 3033.3966\n",
      "Epoch 1929/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 355.0964 - val_loss: 3289.4146\n",
      "Epoch 1930/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 403.6005 - val_loss: 3221.2988\n",
      "Epoch 1931/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 389.3446 - val_loss: 3087.0352\n",
      "Epoch 1932/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 380.0210 - val_loss: 3093.7480\n",
      "Epoch 1933/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 397.7892 - val_loss: 3161.9797\n",
      "Epoch 1934/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 2ms/step - loss: 364.5831 - val_loss: 3182.9548\n",
      "Epoch 1935/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 375.0111 - val_loss: 3256.5268\n",
      "Epoch 1936/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 372.0229 - val_loss: 3107.2526\n",
      "Epoch 1937/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 348.7489 - val_loss: 3165.6493\n",
      "Epoch 1938/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 358.7027 - val_loss: 3052.7826\n",
      "Epoch 1939/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 389.1746 - val_loss: 3111.2517\n",
      "Epoch 1940/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 374.0871 - val_loss: 3283.4847\n",
      "Epoch 1941/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 350.4986 - val_loss: 3134.4405\n",
      "Epoch 1942/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 361.5374 - val_loss: 3108.3112\n",
      "Epoch 1943/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 347.8983 - val_loss: 3184.5938\n",
      "Epoch 1944/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 367.5125 - val_loss: 3247.1915\n",
      "Epoch 1945/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 344.0959 - val_loss: 3107.7323\n",
      "Epoch 1946/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 373.8490 - val_loss: 3231.4676\n",
      "Epoch 1947/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 380.8504 - val_loss: 3211.6614\n",
      "Epoch 1948/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 395.4193 - val_loss: 3083.1093\n",
      "Epoch 1949/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 363.9809 - val_loss: 3098.0773\n",
      "Epoch 1950/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 343.4937 - val_loss: 3294.5871\n",
      "Epoch 1951/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 370.8803 - val_loss: 3195.8119\n",
      "Epoch 1952/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 377.3396 - val_loss: 3136.0683\n",
      "Epoch 1953/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 376.4292 - val_loss: 3117.1954\n",
      "Epoch 1954/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 342.6844 - val_loss: 3329.9603\n",
      "Epoch 1955/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 347.6789 - val_loss: 3347.4458\n",
      "Epoch 1956/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 349.8433 - val_loss: 3126.4838\n",
      "Epoch 1957/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 343.8336 - val_loss: 3159.9063\n",
      "Epoch 1958/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 354.1018 - val_loss: 3246.5429\n",
      "Epoch 1959/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 324.8088 - val_loss: 3241.4045\n",
      "Epoch 1960/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 356.7341 - val_loss: 3141.9952\n",
      "Epoch 1961/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 381.5356 - val_loss: 3321.5570\n",
      "Epoch 1962/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 372.8801 - val_loss: 3230.0671\n",
      "Epoch 1963/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 388.0876 - val_loss: 3222.9148\n",
      "Epoch 1964/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 348.2344 - val_loss: 3167.8555\n",
      "Epoch 1965/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 328.8561 - val_loss: 3213.2936\n",
      "Epoch 1966/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 365.9658 - val_loss: 3147.9468\n",
      "Epoch 1967/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 355.0253 - val_loss: 3253.6845\n",
      "Epoch 1968/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 380.1619 - val_loss: 3176.9124\n",
      "Epoch 1969/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 331.4535 - val_loss: 3257.0945\n",
      "Epoch 1970/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 399.0279 - val_loss: 3152.8873\n",
      "Epoch 1971/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 400.7091 - val_loss: 3193.6734\n",
      "Epoch 1972/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 439.2474 - val_loss: 3042.7522\n",
      "Epoch 1973/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 348.4134 - val_loss: 3117.3864\n",
      "Epoch 1974/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 353.6370 - val_loss: 3235.0026\n",
      "Epoch 1975/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 317.5389 - val_loss: 3104.4904\n",
      "Epoch 1976/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 400.1375 - val_loss: 3221.1679\n",
      "Epoch 1977/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 343.3997 - val_loss: 3246.5464\n",
      "Epoch 1978/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 360.4811 - val_loss: 3320.4235\n",
      "Epoch 1979/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 338.9821 - val_loss: 3132.0433\n",
      "Epoch 1980/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 347.3642 - val_loss: 3389.1064\n",
      "Epoch 1981/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 335.7788 - val_loss: 3179.3717\n",
      "Epoch 1982/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 338.5745 - val_loss: 3253.9985\n",
      "Epoch 1983/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 328.4831 - val_loss: 3217.7064\n",
      "Epoch 1984/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 321.9328 - val_loss: 3303.8990\n",
      "Epoch 1985/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 317.0498 - val_loss: 3302.8260\n",
      "Epoch 1986/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 338.9567 - val_loss: 3280.8101\n",
      "Epoch 1987/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 347.7771 - val_loss: 3159.6388\n",
      "Epoch 1988/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 315.8486 - val_loss: 3269.8054\n",
      "Epoch 1989/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 340.2269 - val_loss: 3339.1026\n",
      "Epoch 1990/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 326.2450 - val_loss: 3250.6046\n",
      "Epoch 1991/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 347.2300 - val_loss: 3199.0224\n",
      "Epoch 1992/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 322.8007 - val_loss: 3332.7350\n",
      "Epoch 1993/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 326.5909 - val_loss: 3175.5366\n",
      "Epoch 1994/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 342.3146 - val_loss: 3291.8307\n",
      "Epoch 1995/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 330.3156 - val_loss: 3229.0645\n",
      "Epoch 1996/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 324.8727 - val_loss: 3205.3184\n",
      "Epoch 1997/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 418.7013 - val_loss: 3120.0947\n",
      "Epoch 1998/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 488.8194 - val_loss: 3301.2216\n",
      "Epoch 1999/2000\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 442.4543 - val_loss: 3220.1764\n",
      "Epoch 2000/2000\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 435.6457 - val_loss: 3065.7302\n",
      "Training Model took:  363.7298319339752\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "d = 0.01\n",
    "model = Sequential()\n",
    "input_shape=(101,29)\n",
    "model.add(Bidirectional(GRU(64, input_shape=(101,29), return_sequences=False)))\n",
    "model.add(Dropout(d))\n",
    "model.add(SelfAttention(64))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(SelfAttention(32))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(SelfAttention(32))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(SelfAttention(16))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(SelfAttention(16))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(SelfAttention(8))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(SelfAttention(8))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(SelfAttention(4))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(SelfAttention(4))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(SelfAttention(2))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(SelfAttention(2))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "#model = load_model('C:/Users/Danny Lee/Desktop/motion hw/final/LSTM model performance/try_best_nodropout/Regression.h5')\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer= opt)\n",
    "start_fitting = time.time()\n",
    "#model.load_weights('C:/Users/Danny Lee/Desktop/motion hw/final/LSTM model performance/try_best_nodropout/Regression_weights.h5'.encode('utf-8'))\n",
    "#model = tf.keras.models.load_model('C:/Users/Danny Lee/Desktop/motion hw/final/LSTM model performance/try_best_nodropout/Regression_weights.h5')\n",
    "model.fit(X_train, y_train, epochs=2000, batch_size=32,validation_data=(X_val, y_val))\n",
    "fitting_model_time = time.time()\n",
    "print('Training Model took: ', fitting_model_time - start_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f013a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 128)               36096     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "self_attention_1 (SelfAttent (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "self_attention_2 (SelfAttent (None, 32)                1088      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "self_attention_3 (SelfAttent (None, 32)                1088      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "self_attention_4 (SelfAttent (None, 16)                288       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "self_attention_5 (SelfAttent (None, 16)                288       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "self_attention_6 (SelfAttent (None, 8)                 80        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "self_attention_7 (SelfAttent (None, 8)                 80        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "self_attention_8 (SelfAttent (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "self_attention_9 (SelfAttent (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "self_attention_10 (SelfAtten (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "self_attention_11 (SelfAtten (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 53,659\n",
      "Trainable params: 53,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b1e49b8b3286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#保存模型和權重\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0msaved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./LSTM model performance/try0613_dropout_adam/Regression.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./LSTM model performance/try0613_dropout_adam/Regression_weights.h5'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# to store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\Ray Lee\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_wrapper\u001b[1;34m(obj, filepath, overwrite, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0msave_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msave_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ray Lee\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ray Lee\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[1;34m(group, layers)\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             param_dset = g.create_dataset(name, val.shape,\n\u001b[1;32m--> 761\u001b[1;33m                                           dtype=val.dtype)\n\u001b[0m\u001b[0;32m    762\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m                 \u001b[1;31m# scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ray Lee\\anaconda3\\envs\\tf2\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ray Lee\\anaconda3\\envs\\tf2\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, name, obj)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m                 \u001b[0mh5o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "model.build(input_shape)\n",
    "print(model.summary())\n",
    "with open('./LSTM model performance/try0613_dropout_adam/modelsummary.txt', 'w') as f:\n",
    "\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "#earlystopping\n",
    "#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, verbose=2, mode='auto', restore_best_weights=True)\n",
    "#model.fit(X_train, y_train, nb_epoch = 1000, batch_size = 32,validation_data=(X_val, y_val),callbacks=[monitor]) #训练模型1000次\n",
    "#epochxxx =  monitor.stopped_epoch+1   #len(model.history.history['loss'])\n",
    "#print(epochxxx)    \n",
    "\n",
    "#保存模型和權重\n",
    "saved = model.save('./LSTM model performance/try0613_dropout_adam/Regression.h5')    \n",
    "model.save_weights('./LSTM model performance/try0613_dropout_adam/Regression_weights.h5')  # to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cea8a172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#画出迭代loss和acc曲线\n",
    "pd.DataFrame(model.history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f19d61aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 122\n",
      "[118.78939745 168.66861876 129.78341446 229.84965674 181.53197002\n",
      " 172.60282366 235.05464629 159.19058835 205.13049282 187.20889899\n",
      " 263.74881331 193.52709064 172.01884908 220.61833869 202.97967082\n",
      " 176.86472853 160.18305672 119.32390315 280.80955029  65.49232707]\n",
      "[ 98.90438  177.0221   120.681915 223.8749   188.11871  175.97728\n",
      " 219.36491  168.05875  210.94339  175.68286  257.25287  187.6469\n",
      " 164.58221  217.71664  210.36215  185.0453   179.82455  106.18799\n",
      " 259.5724    78.278885]\n",
      "[ 19.88501665  -8.35347597   9.10149918   5.97476356  -6.58674336\n",
      "  -3.374456    15.68973235  -8.86815798  -5.81289707  11.52603766\n",
      "   6.49594466   5.88019427   7.43663472   2.9016944   -7.38248128\n",
      "  -8.18057482 -19.64149772  13.13591487  21.2371626  -12.78655782]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nepocha=range(len(y_train))\\nepochb=range(len(y_train_predict))\\nplt.plot(epocha, y_train, \\'g\\', label=\\'real\\')\\nplt.plot(epochb, y_train_predict, \\'b\\', label=\\'predict\\')\\nplt.legend((\\'real\\', \\'predict\\'),fontsize=\\'15\\')\\nplt.title(\"Train Data\",fontsize=\\'30\\') #添加标题\\nplt.show()\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在训练集上的拟合结果\n",
    "\n",
    "y_train_predict=model.predict(X_train)\n",
    "y_train_predict=y_train_predict[:,0]\n",
    "\n",
    "c = range(len(y_train[0:20]))\n",
    "    \n",
    "#categories = ['Category 1', 'Category 2']\n",
    "#x = range(len(categories))\n",
    "a = np.arange(20)\n",
    "b = np.arange(20)\n",
    "print(len(y_train),len(y_train_predict))\n",
    "\n",
    "y_error = np.empty((20))\n",
    "\n",
    "#plt.bar(a,y_train[0:20], label='True Values',color='blue')\n",
    "#plt.bar(b,y_train_predict[0:20],label='Predictions',color='orange')\n",
    "for i in range(len(y_train[0:20])):\n",
    "    y_error[i] =y_train[i]-y_train_predict[i]\n",
    "print(y_train[0:20])\n",
    "print(y_train_predict[0:20])\n",
    "print(y_error)\n",
    "real_values_abs = np.abs(y_error)\n",
    "plt.bar(a,real_values_abs,label='Error',color='orange')\n",
    "\n",
    "#plt.title('title name')\n",
    "plt.xlabel('Training Data',fontsize='15')\n",
    "plt.ylabel('Value(mm)')\n",
    "#plt.title('True Values vs Predictions')\n",
    "plt.title('MAE',fontsize='15')\n",
    "plt.xticks(a, c)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "print(X_train)\n",
    "plt.plot(y_train, label='True Values')\n",
    "plt.plot(y_train_predict, label='Predictions')\n",
    "plt.xlabel('Training Data')\n",
    "plt.ylabel('Value(mm)')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''\n",
    "'''\n",
    "draw=pd.concat([pd.DataFrame(y_train),pd.DataFrame(y_train_predict)],axis=1)\n",
    "draw.iloc[100:150,0].plot(figsize=(12,6))\n",
    "draw.iloc[100:150,1].plot(figsize=(12,6))\n",
    "plt.legend(('real', 'predict'),fontsize='15')\n",
    "plt.title(\"Training Data\",fontsize='30') #添加标题\n",
    "#plt.show()\n",
    "#fig.savefig('./LSTM model performance/Regression_heel_test/train_Validation_loss.png')\n",
    "#展示在训练集上的表现 \n",
    "'''\n",
    "'''\n",
    "epocha=range(len(y_train))\n",
    "epochb=range(len(y_train_predict))\n",
    "plt.plot(epocha, y_train, 'g', label='real')\n",
    "plt.plot(epochb, y_train_predict, 'b', label='predict')\n",
    "plt.legend(('real', 'predict'),fontsize='15')\n",
    "plt.title(\"Train Data\",fontsize='30') #添加标题\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0220f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 122\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#在训练集上的拟合结果\n",
    "\n",
    "y_train_predict=model.predict(X_train)\n",
    "y_train_predict=y_train_predict[:,0]\n",
    "\n",
    "c = range(len(y_train[0:20]))\n",
    "    \n",
    "#categories = ['Category 1', 'Category 2']\n",
    "#x = range(len(categories))\n",
    "a = np.arange(20)\n",
    "b = np.arange(20)\n",
    "print(len(y_train),len(y_train_predict))\n",
    "\n",
    "plt.bar(a,y_train[0:20], label='True Values',color='blue')\n",
    "plt.bar(b,y_train_predict[0:20],label='Predictions',color='orange')\n",
    "\n",
    "#plt.bar(a,y_train[0:20]-y_train_predict[0:20],label='誤差',color='orange')\n",
    "\n",
    "#plt.title('title name')\n",
    "plt.xlabel('Training Data')\n",
    "plt.ylabel('Value(mm)')\n",
    "plt.title('True Values vs Predictions')\n",
    "#plt.title('誤差')\n",
    "plt.xticks(a, c)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07dc0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n",
      "[168.25838062 152.62809137 223.2672003  221.87804557 148.94881106\n",
      " 136.39133154 108.68449668 107.45996203 294.5320811  187.27637083\n",
      " 153.29079323 161.11214318 182.72364021 180.25442954 183.59666396\n",
      " 187.09308248 162.13419856 231.15614497 166.28238229 202.91433836]\n",
      "[164.95781 174.51329 193.07237 209.47978 136.27103 287.77338 142.9574\n",
      " 110.24908 259.48972 181.306   137.21822 124.23182 152.8408  157.28009\n",
      " 179.1677  194.58997 141.51015 192.37854 173.68112 130.77498]\n",
      "[   3.30057117  -21.88519904   30.19482786   12.39826347   12.67778445\n",
      " -151.38204493  -34.27290078   -2.78911481   35.04236553    5.97037107\n",
      "   16.07257728   36.88032403   29.88283515   22.97433921    4.42896987\n",
      "   -7.49688334   20.62405147   38.77760494   -7.39873953   72.13935973]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nepochc=range(len(y_test))\\nepochd=range(len(y_test_predict))\\n\\nplt.plot(epochc, y_test, \\'g\\', label=\\'real\\')\\nplt.plot(epochd, y_test_predict, \\'b\\', label=\\'predict\\')\\nplt.title(\"Test Data\",fontsize=\\'30\\') #添加标题\\nplt.legend((\\'real\\', \\'predict\\'),loc=\\'upper right\\',fontsize=\\'15\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict=model.predict(X_test)\n",
    "y_test_predict=y_test_predict[:,0]\n",
    "\n",
    "c = range(len(y_test[0:20]))\n",
    "print(len(y_test),len(y_test_predict))\n",
    "\n",
    "a = np.arange(20)\n",
    "b = np.arange(20)\n",
    "#print(len(y_train),len(y_train_predict))\n",
    "\n",
    "y_error = np.empty((20))\n",
    "\n",
    "\n",
    "for i in range(len(y_test[0:20])):\n",
    "    y_error[i] =y_test[i]-y_test_predict[i]\n",
    "print(y_test[0:20])\n",
    "print(y_test_predict[0:20])\n",
    "print(y_error)\n",
    "real_values_abs = np.abs(y_error)\n",
    "plt.bar(a,real_values_abs,label='Error',color='blue')\n",
    "plt.xlabel('Test Data',fontsize='15')\n",
    "plt.ylabel('Value(mm)')\n",
    "#plt.title('True Values vs Predictions')\n",
    "plt.title('MAE',fontsize='15')\n",
    "plt.xticks(a, c)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Plot the true values and predictions\n",
    "plt.plot(y_test, label='True Values')\n",
    "plt.plot(y_test_predict, label='Predictions')\n",
    "plt.xlabel('Testing Data')\n",
    "plt.ylabel('Value(mm)')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#在测试集上的预测\n",
    "y_test_predict=model.predict(X_test)\n",
    "y_test_predict=y_test_predict[:,0]\n",
    "draw=pd.concat([pd.DataFrame(y_test),pd.DataFrame(y_test_predict)],axis=1);\n",
    "draw.iloc[200:250,0].plot(figsize=(12,6))\n",
    "draw.iloc[200:250,1].plot(figsize=(12,6))\n",
    "plt.legend(('real', 'predict'),loc='upper right',fontsize='15')\n",
    "plt.title(\"Test Data\",fontsize='30') #添加标题\n",
    "#展示在测试集上的表现 \n",
    "\n",
    "y_test_predict = model.predict(X_test).flatten()\n",
    "y_test_predict=y_test_predict[:,0]\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, y_test_predict)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 100]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)\n",
    "'''\n",
    "'''\n",
    "epochc=range(len(y_test))\n",
    "epochd=range(len(y_test_predict))\n",
    "\n",
    "plt.plot(epochc, y_test, 'g', label='real')\n",
    "plt.plot(epochd, y_test_predict, 'b', label='predict')\n",
    "plt.title(\"Test Data\",fontsize='30') #添加标题\n",
    "plt.legend(('real', 'predict'),loc='upper right',fontsize='15')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca6cf74d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集上的MAE/MSE/MAPE\n",
      "13.16727199736739\n",
      "307.28291977990693\n",
      "7.779874864780738\n",
      "測試集上的MAE/MSE/MAPE\n",
      "29.688922956410924\n",
      "1643.6388066364857\n",
      "17.655638802678922\n",
      "預測漲跌正確: 0.65\n",
      "Training Model took:  363.7298319339752\n",
      "TOTAL time spent 2733.47562456131\n"
     ]
    }
   ],
   "source": [
    "#輸出结果\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_pred - y_true) / y_true)) * 100\n",
    "print('訓練集上的MAE/MSE/MAPE')\n",
    "print(mean_absolute_error(y_train_predict, y_train))\n",
    "print(mean_squared_error(y_train_predict, y_train) )\n",
    "print(mape(y_train_predict, y_train) )\n",
    "print('測試集上的MAE/MSE/MAPE')\n",
    "print(mean_absolute_error(y_test_predict, y_test))\n",
    "print(mean_squared_error(y_test_predict, y_test) )\n",
    "print(mape(y_test_predict,  y_test) )\n",
    "y_var_test=y_test[1:]-y_test[:len(y_test)-1]\n",
    "y_var_predict=y_test_predict[1:]-y_test_predict[:len(y_test_predict)-1]\n",
    "txt=np.zeros(len(y_var_test))\n",
    "for i in range(len(y_var_test-1)):\n",
    "    txt[i]=np.sign(y_var_test[i])==np.sign(y_var_predict[i])\n",
    "result=sum(txt)/len(txt)\n",
    "print('預測漲跌正確:',result)\n",
    "print('Training Model took: ', fitting_model_time - start_fitting)\n",
    "#print('训练时间（秒）:',54.56)\n",
    "\n",
    "end = time.time()\n",
    "print('TOTAL time spent', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8642d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
